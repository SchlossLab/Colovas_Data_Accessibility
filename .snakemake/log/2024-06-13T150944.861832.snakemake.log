Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job stats:
job                  count
-----------------  -------
targets                  1
train_ml_set_seed        1
total                    2

Select jobs to execute...
Execute 1 jobs...

[Thu Jun 13 15:09:45 2024]
localrule train_ml_set_seed:
    input: Data/gt_subset_30.new_seq_data.preprocessed.RDS, Code/trainML.R
    output: Data/ml_results/gt_subset_30/runs/glmnet.2000.new_seq_data.model.RDS, Data/ml_results/gt_subset_30/runs/glmnet.2000.new_seq_data.performance.csv
    jobid: 1
    reason: Missing output files: Data/ml_results/gt_subset_30/runs/glmnet.2000.new_seq_data.model.RDS
    wildcards: datasets=gt_subset_30, method=glmnet, seed=2000, ml_variables=new_seq_data
    resources: tmpdir=/tmp

[Thu Jun 13 15:13:59 2024]
Finished job 1.
1 of 2 steps (50%) done
Select jobs to execute...
Execute 1 jobs...

[Thu Jun 13 15:13:59 2024]
localrule targets:
    input: Data/ml_results/gt_subset_30/runs/glmnet.2000.new_seq_data.model.RDS
    jobid: 0
    reason: Input files updated by another job: Data/ml_results/gt_subset_30/runs/glmnet.2000.new_seq_data.model.RDS
    resources: tmpdir=/tmp

[Thu Jun 13 15:13:59 2024]
Finished job 0.
2 of 2 steps (100%) done
Complete log: .snakemake/log/2024-06-13T150944.861832.snakemake.log
