---
title: "Data Accessibility Paper"

editor: visual

date: last-modified
date-format: medium
prefer-html: true

format:
  pdf:
    documentclass: article
    keep-tex: true
    lof: false
    toc: false
    number-sections: false
    output-file: asm_manuscript.pdf
    geometry: margin=1.0in
fontsize: 11pt
linestretch: 1.75
header-includes:
 - \usepackage[left]{lineno}
 - \linenumbers
 - \modulolinenumbers
 - \usepackage{helvet}
 - \renewcommand*\familydefault{\sfdefault}
 - \usepackage[T1]{fontenc}
execute:
  echo: FALSE
  tidy: TRUE
  eval: TRUE
  warning: FALSE
  message: FALSE
  cache: FALSE
runningtitle: 
runningauthor: Colovas

author:
  - name: Joanna Colovas
    orcid: 0009-0005-9180-442X
    affiliations:
      - ref: micro
  - name: Adena Collens
    affiliations:
      - ref: micro
  - name: Patrick D. Schloss
    orcid: 0000-0002-6935-4275
    email: pschloss@umich.edu
    corresponding: true
    affiliations:
      - ref: micro
      - ref: ccmb
affiliations:
  - id: micro
    name: Department of Microbiology & Immunology, University of Michigan
  - id: ccmb
    name: Center for Computational Medicine and Bioinformatics, University of Michigan
abstract: |
 
#importance: TODO
keywords: data accessibility, data reproducibility, supervised machine learning

bibliography: references.bib
csl: asm.csl
---

-   Abstract

-   Importance

    -   Incentivize authors to publish/make available their original data
    -   Publishing data helps get more use out of research
    -   Helps eliminate file drawer effect as it shows negative data

-   Keywords

    -   Data accessibility
    -   Data reproducibility

```{r}
#load data

#library statements
library(tidyverse)

training_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/new_groundtruth.csv")

all_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/predictions_with_metadata.csv.gz")

n_digits <- 4

# #nsd yes metadata needs to be updated at some point
# nsd_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/nsd_yes_metadata.csv.gz")


```

## Introduction

### Scientific Data as a Public Good

The United States Government spent over two hundred million dollars (USD) in 2024 on research expenditures (@congress2024). The result of all of these investments are data, paid for by taxpayers. Therefore, data are a public good. Public goods, for example public libraries, are able to be used by anyone without barrier to entry, and without diminishing the use of others. Data should be made available to be used by others, and best used as for the benefit of those who provided the funds for it. Generated data are useful in so many ways, not only by the original generators to analyze and answer study questions, but can also be used to answer further questions on the same study system, to replicate the original analyses, and combined into meta-studies, by combining multiple similar datasets. A key tenet of the scientific method is the ability to replicate scientific findings to ensure that they are not due to error. Scientific findings can be replicated is by re-completing the same analyses by another researcher, or by completing another type of analysis on the same data. This is only possible if the data used to complete the original analyses are available for use. Additionally, data can be used to eliminate possible solutions to a problem by the publishing of negative or non-significant data. Thinking of even negative data as a public good, their availability can help researchers avoid sinking time and financial resources into the investigation of non-viable hypotheses. Funding sources are not available to pursue the generation of negative data, and agencies look to support fruitful research. As a result, researchers have few incentives to publish negative or non-significant results. This lack of publication of non-fruitful investigation is more commonly known as the "file drawer effect" (@moniz_filedrawer_2025, @rosenthal_1979).

In opposition to the file drawer effect, data availability (DA) is the practice of making raw experimental data, such as nucleic acid sequences, and analyses publicly accessible, often via upload in maintained databases. These databases are public "commons" and help widely benefit when they are well maintained and well used. With the latest and greatest methodologies available across fields, increasing amounts of data are being generated each day, especially in the biological sciences, large and complex datasets are the new standard. ( @li_bigdata_2014, @pal_bigdata_2020). Availability of large quantities of study data and their associated metadata (data about data) are necessary resources for appropriate use and re-use of data, protocols, as well as recreation of analyses. Data availability (DA) is a deeply important component of the scientific process in the digital age, and curation of digital records is a slowly emerging topic in data science( @howe_2008). Available data and analyses are the gold standard for recreation of studies and replication of their results. Not only is replication a worthy goal, but large datasets are often underutilized, and can continue to provide benefit and resources to researchers via their re-use towards investigating and answering further questions. The National Institutes of Health (NIH) has called for grant proposals for the creation, enhancement, and maintenance, of new and existing data repositories ( @not_data_reuse, @par-23-2).

### Nucleic Acid Sequencing Efforts

Beginning in 1996 with the International Strategy Meeting on Human Genome Sequencing in Bermuda, researchers have prioritized the release of all human genome sequencing information so that it may "maximize its benefit to society" (@maxsonjones2018). The meeting participants agreed that "primary sequence data should be rapidly released", with "sequence assemblies \[to\] be released as soon as possible, in some centres\[sic\], assemblies of greater than 1 kb would be released automatically on a daily basis", and that "finished annotated sequence should be submitted immediately to public databases" (@maxsonjones2018). The "Bermuda Principles," as they became known, have been embraced by the large scale Human Genome Project (HGP) since 1998. The HGP endeavored to publish the first complete human genome. In 2003, another meeting, held in Ft. Lauderdale, FL, re-affirmed the 1996 Bermuda Principles, expanded upon them to apply more broadly towards sequencing data, and called for further support of these practices (@maxsonjones2018). These foundational agreements set the stage for both the HGP and the Human Microbiome Project (HMP) to generate and share massive amounts of data over the course of their studies (@proctor_integrative_2019, @gevers_bioinformatics_2012, @group_evaluation_2012). The goal of the HMP was to sequence all body sites to determine the microbes found on and in the human body.

Starting with major projects such as the HGP and HMP, nucleic acid sequencing efforts have been commonly uploaded and released using public databases. This allows for researchers to use and re-use the data from the HGP and HMP. There are three major databases worldwide to support sequencing and sharing efforts. The National Library of Medicine's (NLM) National Center for Biotechnology (NCBI) in the United States, the Research Organization of Information Systems' (ROIS) National Institute of Genetics (NIG) in Japan, and the European Molecular Biology Lab's (EMBL) European Bioinformatics Institue (EBI) in Europe. These three databases are part of the International Nucleotide Sequence Database Collaboration (INSDC) (@indsc) . These large databases make comparative research possible. Genetic lineages of microbes are determined by creating phylogenetic trees which compare a new sequence to existing sequences. Phylogenetic trees show how closely related a new microbial genetic sequence is related to others studied before both in terms of evolution and mutation and in structure and function. Comparative genetics and genomics would not be possible without strong community commitment to data availability.

### Current DA Policies

Current data availability guidelines have been informed by a number of policies created by funding agencies, peer-review journals, conference and special task groups, as well as community interest groups. In 2011, after the Future of Research Communication (FoRC) conference in Germany, scientists and others came together to establish FORCE11, a community interest group which seeks to encourage and promote data availability standards (@force11) . Also in 2011, the Genomic Standards Consortium (GSC) published a set of standards in *Nature Biotechnology* to promote the publication of the "minimum information about a marker gene sequence" (MIMARKS) or "minimum information about x sequence" (MIxS) (@yilmaz_minimum_2011). These standards are checklists usable by data generators and uploaders towards inclusion of relevant data with sequence uploads in the International Nucleotide Sequence Database Collaboration (INSDC). Some checklist items include if the data were published to an INSDC database and metadata about the study systems, data collected, and authors. An important factor is the ability to link the data to the results and to the data generators.

In 2014 the FORCE11 group published the Joint Declaration of Data Citation Principles (JDDCP), a document working towards the standardization of data citation and its future availability(@altman_2015). Some of the JDDCPs include crediting the authors of the data, providing data with unique identifiers, and the persistence of the available data. The Findable, Accessible, Interoperable, and Reuseable (FAIR) data science guiding principles that were put forth in 2016 in *Nature Scientific Data* urges readers to "improve the infrastructure supporting the reuse of scholarly data" (@wilkinson_fair_2016). The FAIR principles are often cited by NIH in funding calls for strong data science practices(@not_data_reuse). In 2021, a *Nature Medicine* publication put forth the "Strengthening the Organization and Reporting of Microbiome Studies" (STORMS) checklist to help authors identify report-worthy elements of their data and metadata ( @mirzayi_reporting_2021). Some items on the STORMS checklist include the sequencing method used in the study, the study design, and physical location of the study. Unfortunately, none of these DA principles or checklists are enforceable by any agency. The National Institutes of Health (NIH) began enforcing the "Policy for Data Management and Sharing" (NOT-OD-21-013) in January of 2023, requiring all NIH funded studies to submit a data management and sharing plan (DMS) with their funding applications, and comply with their DMS plan after generation and publication of the funded work(@NIH_not-od-21-013_dms). A DMS plan includes detailed descriptions of data that will be generated in a study, related tools, standards, and data preservation plans. Non-compliance with NOT-OD-21-013 is identified by funding agencies during annual Research Performance Progress Reports (RPPRs), and may impact future funding decisions (@NIH_not-od-21-013_dms). The NIH policy for non-compliance with award terms and conditions varies due to the type of research misconduct, but is clear that they will protect their interests, including placing conditions on awards, preventing future awards, or closer monitoring of award activities. In this time of uncertain funding from the NIH and other funding agencies, it is more important that ever for investigators to maintain continued compliance. This compliance starts with readily available data and manuscripts. We also believe that statements of data "available on request" are not sufficient to be considered available data. One example was published in *Microbiome*, that a reader in search of data may email the corresponding author, with varying results (@langille_available_2018).

With the advent of next generation sequencing, microbiology research has generated large amounts of sequencing data, and it is common to upload sequence data to a public repository as well as to include data in research publications. Because of this, we set out to determine rates of published raw data in the biological sciences, specifically microbiology, by examining the American Society for Microbiology's (ASM) library of published primary research journals. We were also interested in the relationship between DA and number of citations received by a published work.

### American Society for Microbiology Journals (ASM)

The American Society for Microbiology(ASM) is the major professional body recognized by microbiologists. They have eighteen journals, thirteen primary research journals, three review journals, and two archive journals. In addition, several journals have been folded into others or renamed over time. The ASM family of journals requires that authors "make data fully available, without restriction, except in rare circumstances" (@Asm_opendata). They have adapted this policy from journals *Microbial Genomics* and *PLOS*. In the ASM open data policy they describe the use of a "Data Availability Statement" which includes "data description, name(s) of the repositories, and digital object identifiers (DOIs) or accession numbers" and encourages publishing data on relevant public repositories (@Asm_opendata). Consequences of non-compliance to the ASM open data policy include contacting research article authors to inform of non-compliance, publication of an "Expression of Concern" for the author and their compliance issues, sanctions on publication in ASM journals, as well as contacting the affiliated research institution and/or funding agencies of the authors (@asm_ethics_2025). We endeavor to evaluate how well the microbiology community is using reproducible data practices as we believe that this group of researchers will be early adopters of the technologies available as a result of both the ASM and NIH policies towards data availability.

### Data Avalability Case Studies

The availability of datasets also allows new questions to be answered with existing data or the combination of multiple datasets, such as the use of the Human Microbiome Project's (HMP) sequencing data by researchers to create over 650 scientific publications (@HMP_2025), and the completion of metadata studies, including those efforts participated in by these authors ( @ding_dynamics_2014, @abubucker_metabolic_2012, @huttenhower_structure_2012).

An important tool for creating phylogenies is the NCBI Basic Local Alignment Search Tool (BLAST) (@altschul1990). The BLAST algorithm allows users to compare a nucleic acid or protein sequence to the NCBI database of over 1TB of data to find similar and related sequences. Without the upload of sequences to the NCBI database, the use and success of BLAST would not be possible, despite the effort required on part of the researcher to upload of sequences to one of the INSDC databases.

Availablity of data contributed to the rapid sequencing of the SARS-CoV-2 virus during the 2020 pandemic and subsequent expedition of vaccine development (@maxmen_2021).

With microbiologists commonly uploading nucleic acid sequences to public databases, the aim of this study was to determine the current state of data availability in twelve primary research journals from the ASM family of journals. Primary research articles were classified with using two machine learning models to answer two questions; "Does this paper contain new sequence data?", and "Is the data available?" Once these questions were answered, we moved to statistical analyses to answer these and further questions, such as "How does making my data available impact my citation metrics over time?" We were interested in citation metrics as a concrete metric to examine how making DA benefits researchers and as a possible incentive towards making DA. Our hypotheses were that microbiologists would have fairy high rates of DA given the field's reliance on comparative research, and that other fields such as immunology would have lowered rates of DA, as well as the hope that papers with DA would have a greater number of citations.

## Results

### General Description of the Experiment

We set out to determine the current state of data availability in twelve primary research journals from the ASM family of journals. This objective was completed by first acquiring all papers published in the journals of interest between 2000 and 2024 using the Crossref database and command line tools. We then trained two random forest machine learning models to differentiate if each paper 1), contained "New Sequencing Data" (NSD) and 2) if the paper had "Data Available" (DA). To avoid overfitting the models, we trained each model multiple times, performing validations on a subset of data after each iteration. This allowed us to have a greater number of papers in the training dataset, as well as to have great accuracy and precision within our models. Using our trained models, we were able to classify over 150,000 papers from the whole dataset to determine if they were NSD or contained DA. After this, we could perform statistical modeling to describe the data, and generate summary statistics. We were especially interested in the ways in which NSD and DA impact citation metrics.

### ASM Journals

We used twelve of the ASM primary research journals in this study. Of note, several journals had changes to their publication goals during the 2000-2024 time period. The *Journal of Bacteriology* was the primary journal to publish new genome announcements until 2013 when ASM announced journal *Genome Announcements* as a more permanent home for this type of data. *Genome Announcements* was active from 2013 until 2018, when it was re-branded to *Microbiology Resource Announcements*, which has been active from 2018 until present. These two journals appear separately in our analyses due to the organization of the Crossref database. Not all journals are equally likely to contain NSD and have sequencing DA as a result of their field of interest. Journals publishing new genome announcements have a high percentage of NSD papers, and a high percentage of DA within those papers. As a result, since the introduction of *Genome Announcements* and *Microbiology Resource Announcements*, the *Journal of Bacteriology* has had fewer NSD papers, and fewer papers with DA since 2013. Another journal of note is *Microbiology Spectrum* and its re-brand. From 2013 until the fall of 2021, *Microbiology Spectrum* was a review journal. After this point, *Microbiology Spectrum* became a primary research journal (@asm_relaunch_2021). Review journals are less likely to publish articles with NSD, and to have DA. Several journals, including *Microbiology Spectrum*, do not span the entire time period for the study. Journals *mBio* (b.2010), *Microbiology Spectrum* (b. 2013, re-brand 2021), *mSphere* (b. 2016), *mSystems* (b. 2016), and *Genome Announcements* (2013-2018). Journals *Antimicrobial Agents and Chemotherapy; Infection and Immunity;* and the *Journal of Microbiology and Biology Education;* are all less likely to contain NSD and have DA than the other journals in the dataset. Therefore, aggregate measures of all journal activity together are thought to be less accurate than those depicting each journal separately.

### Descriptive Statistics

```{r make_data_tables}

training_data_table <-
training_data %>% 
    count(container.title, new_seq_data, data_availability) %>%
    complete(., container.title, new_seq_data, data_availability, fill = list(`n` = 0L)) %>%
    mutate(.by = container.title, 
               n_total = sum(`n`), 
           n_fract = (n_total/sum(.$`n`)*100))  %>% 
    filter(new_seq_data == "Yes")  %>%
       mutate(.by = container.title,
           n_nsd = sum(`n`), 
           fract_nsd = n_nsd/n_total*100) %>%
    filter(data_availability == "Yes") %>% 
    mutate(.by = container.title, 
          n_da = `n`, 
          fract_da = n_da/n_nsd*100) %>%
    select(container.title, n_total:fract_da) # i think the NaN is fine for now

# kableExtra::kable(training_data_table, caption = "Summary Statistics of Model Training Dataset", 
#                 digits = n_digits)

#let's make a graph instead
# whole_fract_nsd <- 
    ggplot(training_data_table, aes(x = fract_nsd, y = container.title)) + 
        geom_point()  
        #labs(title = 'Fraction of papers with NSD \n in Training Dataset')

# whole_fract_da <- 
    ggplot(training_data_table, aes(x = fract_da, y = container.title)) + 
        geom_point() 
        #labs(title = 'Fraction of papers with NSD \n and DA in Training Dataset')


 
#whole dataset yay!
whole_dataset_table <-
all_data %>% 
    count(container.title, nsd, da) %>%
    na.omit() %>%
    complete(., container.title, nsd, da, fill = list(`n` = 0L)) %>%
    mutate(.by = container.title, 
               n_total = sum(`n`), 
           n_fract = (n_total/sum(.$`n`)*100))  %>% 
    filter(nsd == "Yes")  %>%
       mutate(.by = container.title,
           n_nsd = sum(`n`), 
           fract_nsd = n_nsd/n_total*100) %>%
    filter(da == "Yes") %>% 
    mutate(.by = container.title, 
          n_da = `n`, 
          fract_da = n_da/n_nsd*100) %>%
    select(container.title, n_total:fract_da)

# kableExtra::kable(whole_dataset_table, caption = "Summary Statstics of All Data", 
#                 digits = n_digits)

# t_fract_nsd <- 
    ggplot(whole_dataset_table, aes(x = fract_nsd, y = container.title)) + 
        geom_point() 
        #labs(title = 'Fraction of papers with NSD \n in Whole Dataset')
# t_fract_da <- 
    ggplot(whole_dataset_table, aes(x = fract_da, y = container.title)) + 
        geom_point() 
        #labs(title = 'Fraction of papers with DA \n in Whole Dataset')



```

```{r whole_dataset_stats}
#whole dataset



## whole dataset stats
n_total <- nrow(all_data)
all_per_nsd <- count(all_data, nsd) %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 3]]

all_per_da <- count(all_data, nsd, da) %>%
            filter(nsd == "Yes") %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 4]]


#year distribution - do we want a figure for this or the table? 
# do we want to combine this with the training set data? 
all_years <- 
   all_data %>% 
    count(., year.published) %>%
    rename(., n_whole_dataset = `n`)



#journal w/ highest and lowest rate of nsd
all_fract_high_nsd <- max(whole_dataset_table$fract_nsd)
all_j_high_nsd <- whole_dataset_table[[which.max(whole_dataset_table$fract_nsd), 1]]
all_fract_low_nsd <- min(whole_dataset_table$fract_nsd)
all_j_low_nsd <- whole_dataset_table[[which.min(whole_dataset_table$fract_nsd), 1]]

#journals w/ high/low rate da - two journals have 100% DA
all_fract_high_da <- max(whole_dataset_table$fract_da, na.rm = TRUE)
all_j_high_da <- whole_dataset_table[whole_dataset_table$fract_da == all_fract_high_da, 1] %>%
               stringr::str_flatten(, collapse = " and ", na.rm = TRUE)

all_fract_low_da <- min(whole_dataset_table$fract_da, na.rm = TRUE)
all_j_low_da <- whole_dataset_table[[which.min(whole_dataset_table$fract_da), 1]]


## avg citations/paper
all_total_avg_cites <- all_data %>% 
    select(file, container.title, is.referenced.by.count) %>% 
    summarize(container.title = "Total",  
              mean_refs = mean(is.referenced.by.count, na.rm = TRUE), 
               median_refs = median(is.referenced.by.count, na.rm = TRUE))

all_avg_cites_j <- all_data %>% 
    select(file, container.title, is.referenced.by.count) %>% 
    summarize(.by = container.title, 
              mean_refs = mean(is.referenced.by.count, na.rm = TRUE), 
               median_refs = median(is.referenced.by.count, na.rm = TRUE))
all_avg_cites <- rbind(all_avg_cites_j, all_total_avg_cites)

all_max_median_cites <- max(all_avg_cites_j$median_refs, na.rm = TRUE)
all_j_max_median_cites <- all_avg_cites_j[[which.max(all_avg_cites_j$median_refs), 1]]

all_min_median_cites <- min(all_avg_cites_j$median_refs, na.rm = TRUE)
all_j_min_median_cites <- all_avg_cites_j[[which.min(all_avg_cites_j$median_refs), 1]]
```

#### Whole Dataset

Using the Crossref database of DOIs, with validation from the Web of Science, NCBI, and Scopus DOI databases, we downloaded N = `{r} n_total` unique records of papers published in ASM journals between January 1st, 2000 and December 31st, 2024. These papers came from journals *Applied and Envrionmental Microbiology; Antimicrobial Agents and Chemotherapy; Infection and Immunity; Journal of Clinical Biology; Journal of Virology; Journal of Bacteriology; Journal of Microbiology and Biology Education; Microbiology Resource Announcments* (formerly known as *Genome Announcements*); *mSystems; mSphere; mBio; and Microbiology Spectrum.* After downloading the HTML content of each paper, we cleaned the HTML content and readied it to apply our machine learning models to classify each paper. Overall, `{r} round(all_per_nsd, n_digits)`% of papers had NSD, and `{r} round(all_per_da, n_digits)`% of papers with NSD, had DA. See Figure 2XX for percentages of NSD and DA for each journal. The journal with the highest rate of NSD was `{r} all_j_high_nsd` at `{r} round(all_fract_high_nsd, n_digits)`%, and the lowest was `{r} all_j_low_nsd` at `{r} round(all_fract_low_nsd, n_digits)`%. The journal with the highest rate of DA was \*`{r} all_j_high_da`\* at `{r} round(all_fract_high_da, n_digits)`%, and the lowest was \*`{r} all_j_low_da`\* at `{r} round(all_fract_low_da, n_digits)`%. This was expected as \*`{r} all_j_high_nsd`\* publishes mainly new genomic sequence data and makes the data available. On average, papers in the dataset had a median of `{r} round(all_total_avg_cites$median_refs, n_digits)` citations/article. This number varies by journal, see table XXXX for data by journal. The journal with the highest median rate of citations/article was \*`{r} all_j_max_median_cites`\* at `{r} round(all_max_median_cites, n_digits)`%, and the lowest was \*`{r} all_j_min_median_cites`\* at `{r} round(all_min_median_cites, n_digits)`%. The journals in the dataset span years 2000-2024. See table XXXX for the distribution of papers per year in the dataset.

```{r training_data_stats}
#ok now we need to find the percentages/data that we need 

## Training dataset stats

n_training <- nrow(training_data)
t_per_nsd <- count(training_data, new_seq_data) %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 3]]

t_per_da <- count(training_data, new_seq_data, data_availability) %>%
            filter(new_seq_data == "Yes") %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 4]]


#journal w/ highest and lowest rate of nsd
t_fract_high_nsd <- max(training_data_table$fract_nsd)
t_j_high_nsd <- training_data_table[[which.max(training_data_table$fract_nsd), 1]]
t_fract_low_nsd <- min(training_data_table$fract_nsd)
t_j_low_nsd <- training_data_table[[which.min(training_data_table$fract_nsd), 1]]

#journals w/ high/low rate da - two journals have 100% DA
t_fract_high_da <- max(training_data_table$fract_da, na.rm = TRUE)
t_j_high_da <- training_data_table[training_data_table$fract_da == t_fract_high_da, 1] %>%
               stringr::str_flatten(, collapse = " and ", na.rm = TRUE)

t_fract_low_da <- min(training_data_table$fract_da, na.rm = TRUE)
t_j_low_da <- training_data_table[[which.min(training_data_table$fract_da), 1]]


## avg citations/paper
t_total_avg_cites <- training_data %>% 
    select(paper, container.title, is.referenced.by.count) %>% 
    summarize(container.title = "Total",  
              mean_refs = mean(is.referenced.by.count), 
               median_refs = median(is.referenced.by.count))

t_avg_cites_j <- training_data %>% 
    select(paper, container.title, is.referenced.by.count) %>% 
    summarize(.by = container.title, 
              mean_refs = mean(is.referenced.by.count), 
               median_refs = median(is.referenced.by.count))
t_avg_cites <- rbind(t_avg_cites_j, t_total_avg_cites)

t_max_median_cites <- max(t_avg_cites_j$median_refs, na.rm = TRUE)
t_j_max_median_cites <- t_avg_cites_j[[which.max(t_avg_cites_j$median_refs), 1]]

t_min_median_cites <- min(t_avg_cites_j$median_refs, na.rm = TRUE)
t_j_min_median_cites <- t_avg_cites_j[[which.min(t_avg_cites_j$median_refs), 1]]

```

#### Training Dataset

We created a subset of the whole dataset to train our machine learning models. The training dataset initially had N = 500 papers, but was increased over time due to gaps in the dataset, and after subsequent validation of the trained models (see below), a total of N = `{r} round(n_training, n_digits)`. XXSee Figure 1 for the distribution of papers per journal in the training dataset.XX The journals in the dataset also span years 2000-2024. See Figure 5XX for the distribution of papers per year in the whole and training datasets. Overall, `{r} round(t_per_nsd, n_digits)`% of papers had NSD, and `{r} round(t_per_da, n_digits)`% of papers with NSD, had DA. See Figures 3 and 4 for percentages of NSD and DA for each journal. The journal with the highest rate of NSD was \*`{r} t_j_high_nsd`\*at `{r} round(t_fract_high_nsd, n_digits)`%, and the lowest was \*`{r} t_j_low_nsd`\* at `{r} round(t_fract_low_nsd, n_digits)`%. The journals with the highest rate of DA in NSD papers were \*`{r} t_j_high_da`\* at `{r} round(t_fract_high_da, n_digits)`%, and the lowest was \*`{r} t_j_low_da`\* at `{r} round(t_fract_low_da, n_digits)`%. On average, papers in the dataset had median `{r} round(t_total_avg_cites$median_refs, n_digits)` citations/article. This number varies by journal, see table XXXX for data by journal. The journal with the highest rate of citations/article was \*`{r} t_j_max_median_cites`\* at `{r} round(t_max_median_cites, n_digits)`, and the lowest was \*`{r} t_j_min_median_cites`\*at `{r} round(t_min_median_cites, n_digits)`.

### Year Published Distribution Table

```{r}
#year distribution - do we want a figure for this or the table? 
t_years <- 
    training_data %>% 
    count(., year.published) %>%
    rename(., n_training_dataset = `n`)

year_pub_table <-
full_join(all_years, t_years, by = join_by(year.published)) 

kableExtra::kable(year_pub_table, caption = "Distribution of Year Published for Whole and Training Dataset", 
                        digits = n_digits)

# whole_year_pub_graph <-
ggplot(year_pub_table, aes(x = year.published, y = n_whole_dataset)) + 
    geom_point() 
   # labs(title = "Number of published papers \n per year in whole dataset")

```

### Descriptive Statisitcs about the Trained Models

```{r trained_model_stats}
best_models <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/best_model_stats.csv")

kableExtra::kable(best_models, caption = "Trained Model Summary Statistics", 
                    digits = n_digits)

```

#### Figures for trained models

![](../Figures/ml_results/groundtruth/rf/auroc.new_seq_data.png) ![](../Figures/ml_results/groundtruth/rf/auroc.data_availability.png) !["Mtry Performance New Sequencing Data"](../Figures/ml_results/groundtruth/rf/hp_perf.rf.new_seq_data.png) !["Mtry Performance Data Availability"](../Figures/ml_results/groundtruth/rf/hp_perf.rf.data_availability.png)

Two random forest (rf) models were trained to predict if published scientific papers "contained new sequence data" (NSD), and if the paper "had data available" (DA), one model for each variable. Other models such as generalized linear regression (GLM) and boosted trees (XGBoost) were explored, but were ultimately discarded in favor of the random forest model (data not shown). Random forest models were chosen to aid in this classification problem as the creation of many decision trees helps to improve accuracy and precision. This type of model has one hyperparameter, 'mtry' or the number of predictors to be sampled at each decision. During iterative model training, a subset of papers were validated after each completed training and deployment of each model. Papers from each journal, and extras from certain journals were hand-validated against model predictions to generate confusion matrices. Confusion matrices for the final version of each trained model are available in YYYY table(supplement?). To evaluate the fit of the model, we used the Area Under the Receiver Operator Curve (AUROC) which indicates how well the model classifies the data. An AUROC of 0.5 is a random 50/50 guess, and an AURCO of 1.0 indicates that the model always classifies a new item correctly. The NSD model used an mtry value of `{r} round(best_models[[1, 3]], n_digits)` had an Area Under the Curve(AUC) of `{r} round(best_models[[3, 3]], n_digits)` and an accuracy of `{r} round(best_models[[5, 3]], n_digits)`. The sensitivity of the NSD model was `{r} round(best_models[[8,3]], n_digits)`, and the specificity of the model was `{r} round(best_models[[9,3]], n_digits)`. The DA model used an mtry value of `{r} round(best_models[1,2], n_digits)` had an AUC of `{r} round(best_models[[3,2]], n_digits)` and an accuracy of `{r} round(best_models[[5,2]], n_digits)`. The sensitivity of the DA model was `{r} round(best_models[[8,2]], n_digits)`, and the specificity of the model was `{r} round(best_models[[9,2]], n_digits)` (See Table 4XX for more information on trained machine learning models). This shows that the models fit the data well, and can provide classifications on new data with an expected error rate of less than 10%. We deemed this as acceptable, accounting for variability in papers and data, as well as the large size of the dataset on which we deployed the models.

### Regression Model using Negative Binomial Models

```{r neg_binomial, eval = FALSE}

neg_bin <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/negative_binomial/nsd_yes_glmnb_coeftable.csv")

neg_bin_table <- 
   kableExtra::kable(neg_bin, caption = "Coefficients for Negative Binomial Modeling", 
    digits = n_digits) 

```

In this study we sought to investigate the effect of NSD and DA on the number of citations received by a given paper. We focused on NSD papers to determine the effect of having DA. This led us to the use of a negative binomial regression model to best describe our data. All regression data had NSD (NSD == "Yes"). We focused on the continuous outcome of "number of citations" with predictor variables journal (categorical), age in months (continuous), and DA status (dichotomous). Due to the number of citations being bell shaped with a long right tail (very few papers at advanced age with many citations), the model that best described our data was a negative binomial regression model. A negative binomial model is appropriate for data that begins at zero and has a long 'tail' of data, as well as has differing means per group. This model also includes a dispersion parameter, $\theta$ to describe the spread of the data. We applied a log transformation to our age variable (age.in.months) to better describe the relationship between time and number of citations received. See supplemental table XXX6 for model coefficients. In general, we found that NSD papers that made DA received more citations over time than those that did not. See figure XXXX for trends in each major journal. ??XX In Figure 7XX, we have calculated the ratio of number of citations for papers of similar age containing data vs those that do not $(Citations[DA=Yes])/(Citations[DA=No])$. Over time, papers with DA receive more citations than those without up to well over 1.5x in some journals (*Journal of Clinical Microbiology*). In all journals excepting the *Journal of Bacteriology*, papers with DA have a greater number of citations at time point 60 months after publication, if not sooner via ratio plot. Over time this ratio increases further, demonstrating increased citations for papers with DA available over time. Figure 8XX shows that the gap between predicted number of citations for papers with DA vs those without widens over time, even beyond the width of the 95% confidence interval.

#### Figures for Negative Binomial

!["Ratio of DA Yes to DA No"](../Figures/negative_binomial/emmeans_contrast_plot.png) ![](../Figures/negative_binomial/model_predicted_plot.png)

## Discussion

We investigated the impact of data availability on citation metrics in new sequencing papers by deploying machine learning models on over 150,000 papers from the ASM family of journals. Overall, making data available increases citation metrics over time, an added benefit to authors for the effort of making their data available.

On average, more than half of papers with NSD had DA (`{r} round(all_per_da, n_digits)`%), showing that authors of NSD papers are more often than not, making their data publicly available. This is in line with recent NIH policy requiring that data be made available using the Data Management and Sharing Plan (DMS plan) outlined in the NIH's NOT-OD-21-013 (@NIH_not-od-21-013_dms). This NIH policy went into effect in 2023. XXDo we want percent for 2024?XX Expectledly, journals *Genome Announcements* and *Microbiology Resource Announcements* had the highest rates of DA in NSD papers. These journals publish primarily new genomic sequences and are required by ASM to make data available. Journals such as the *Journal of Microbiology and Biology Education* and *Infection and Immunity* which publish fewer NSD papers due to their specific subject matters, have lower rates of NSD and therefore DA in their journals.

Next, we looked further into the impact of DA on citation metrics using a negative binomial regression model. Using this model we found that over time, papers with DA receive more citations than those without up to well over 1.5x the amount of citations in some journals (Fig XXXX. *Journal of Clinical Microbiology*). This effect intensifies over time, with the greatest differences in citations occurring at the 108 months since publication time point. This is great news to manuscript authors, that simply making their DA can provide as much as 50% increase in citations over time. We believe that this more than justifies the work of making data available. We hope that these data will help to incentivize authors to make data available.

We acknowledge the limitations of our study data, that by focusing only on papers published in the ASM family of journals, our results will not be as generalizable. We understand that this relationship between DA and citation metrics may not be as strong in other families of journals, but we hope with the newest NIH funding policies, these trends will continue to improve DA. Another limitation is the availability of paper metadata from various databases, with Crossref having the most complete metadata available for each paper. We were also not able to track citation metrics for individual papers over time. Our database sources only had available citation metrics at the time of dataset download (February 2025), with no intermediate time points for any given paper. This leaves us unable to understand trends over time due to popularity or obsolescence of technique or results.

While making DA in publications has a citation advantage, we hope that is not the only reason authors choose to make their data available. The need for reproducibility in science and demand for large datasets are additional reasons, as well as the NIH funding requirement. We hope that authors recognize the need to contribute to the data "commons", to further work done by others, and that even their negative results have value and power to stop others from continuing down dead ends.

## Materials and Methods

### Preparation of the Larger Experimental Dataset

To fully answer our research questions, we created a larger dataset with N = 155779 papers curated from reference databases Crossref, NCBI, Scopus, and the Web of Science (@crossref, @ncbi, @scopus, @wos). These papers span all twelve ASM journals of interest from January 1st, 2000 to December 31st, 2024. The ASM Journals of interest were *Applied and Envrionmental Microbiology; Antimicrobial Agents and Chemotherapy; Infection and Immunity; Journal of Clinical Biology; Journal of Virology; Journal of Bacteriology; Journal of Microbiology and Biology Education; Microbiology Resource Announcments* (formerly known as *Genome Announcements*); *mSystems; mSphere; mBio; and Microbiology Spectrum.* The data was updated as of February 10th, 2025 with all citation counts frozen at that date.

### Creation of the Training set

To train our random forest machine learning model, we first created an appropriate training data set. For our initial training set, we chose an initial set of papers from across each journal and the time period of interest, adding special emphasis to include papers that were part of our desired set of interest (i.e. contained published data) to ensure that our two models could adequately characterize each paper as a new sequencing paper and if it published raw sequencing data or not. After creating our initial dataset, it was necessary to identify the status of both variables by hand and determine if each paper contained "new sequencing data" (NSD), and if each one had "data available" (DA). This was completed by opening each paper in an internet browser window, and searching for a "data availability" or similar statement. See Table 1XXX for specific cases and how each of these cases were identified for the purpose of this study.

| Scenario | NSD Status | DA Status |
|----------------------------------|-------------------|-------------------|
| Paper is not about generating new sequencing data | No | No |
| Paper is about generating new sequencing data but has no data available | Yes | No |
| Paper is about generating new sequencing data and has data available | Yes | Yes |
| Paper uses sequencing as a confirmation of experimental technique (i.e. confirmation of plasmid insertion) | No | No |
| Paper discusses new computational or experimental tools | No | No |
| Paper has microarray data | No | No |
| Papers using MLST ONLY | No | No |
| Papers using qPCR ONLY | No | No |
| Papers about protein sequencing that have nucleotide sequencing | Yes | Yes/No depending on DA |
| Papers using iRNA techniques | No | No |
| Papers using pyrosequencing/454 techniques | Yes | Yes/No depending on DA |

: Possible Data Scenarios

### Adding Additional Training Set Papers

After initial training of our random forest models, a random sampling of papers was collected for each journal to audit the efficacy of the models. To audit the efficacy of the models, we hand identified the status of both variables of interest, NSD and DA. We looked for weaknesses in the models, and updated methodology to reflect important areas of interest. For example, in 2023 the ASM journals changed their formatting to include the data availability statement of a paper in a sidebar of the webpage. We identified this by noticing that all papers from journal *Microbiology Resource Announcements* from 2023-2024 were incorrectly characterized by the model as DA = No. The sidebar of the webpage was not included in the text the model was considering, and code had to be updated to include all sidebar data for all papers. These improvements to the model created a larger and more comprehensive training set of N = `r n_training`. These validations allowed us to create confusion matrices for each model. Confusion matrices for the final version of each trained model are available in XXX table(XXsupplement?).

### Creation of the Training Data from Training dataset

To perform the computational steps required for these experiments, we used the python tool Snakemake ( @mölder_snakemake ), and the University of Michigan's high performance computing cluster (see acknowledgements). Using our selected papers from the training dataset, we downloaded the entirety of each paper's source HTML using the command line tool wget. This allowed us to use the source HTML multiple times for updated analyses without the need to re-query the ASM webservers numerous times. Next, we performed cleaning of the HTML using R packages rvest (@rvest), textstem( @textstem), and xml2 (@xml2) to get the desired portions of the paper from the HTML including the abstract, the body of paper, all tables and figures with captions, as well as the side panels for all papers, but especially those containing the data availability statements in papers published after the 2023 change in webpage format (see above). Then we removed unnecessary text using R packages tm(text manipulation)(@tm_article, @tm_manual) and textstem (@textstem), as well as converting all text to lowercase, and the removal of digits and non-alphabetic characters such as whitespace. To have the fewest number of unique words, we lemmatized (sort words by grouping inflected or variant forms of the same word) words to trace them back to their root words and eliminate any possible issues with word tense. After this, we created and counted our 'tokens', phrases of up to 1-3 consecutive words from the text of the paper using R package tokeinziers (@tokenizers). Towards the goal of the fewest meaningful number of words, we used the 'Snowball' (@stopwords) dictionary of 'stop words' to remove non-meaningful words such as articles 'a', 'an', and 'the'. We removed the 'space' character with an underscore in multi-word tokens for ease of processing, and created a count table for the tokens in each paper.

Once the tokens in each paper were counted, we transformed the data into a sparse matrix format useable by the R package mikropml (@mikropml), using R packages caret and dplyr (@caret, @dplyr, @tidyverse). Tokens were filtered to those which appear in greater than one paper. This allows comparison between papers by the model. We removed near zero variants (tokens with frequency very close to zero) as well as collapsing perfectly correlated tokens (tokens that always appear together) using R packages caret and mikropml to reduce model complexity. The data was then simplified to keep only the following variables; tokens, frequency, journal information, and hand identified NSD and DA variables. This simiplified sparse matrix data had the mean and standard deviation calculcated and saved for the frequency of each token to later apply a z-scoring method to future data to be predicted by the model.

### Training of the DA and NSD Models

We trained two random forest machine learning models using mikropml's "run_ml" function, one to determine if a paper contained new sequence data (NSD), and another to determine if the paper had data availabile (DA). The mikropml "run_ml" function uses methodology described by \*Topcuoglu et al.\*(@topcuoglu_mikropml_2021) to split data for model training. Random forest models have one hyperparameter to tune, the mtry value. We began with mtry values of 100, 200, 300, 400, 500, and 600, to find peak hyperparameter performance given *N tokens*. We trained the models multiple times in accordance with existing methodologies, first to find the optimal Area Under the Receiver-Operator Curve (AUROC) value for each model with N=100 seeds. Then to find the best mtry performance for each model, with N=1 seed. Finally, with N=1 seed to train each final model for use on experimental data.

### Deploying the RF Models

Once our RF models were ready we applied the same steps to ready papers for application of machine learning models as the model training dataset. See above for descriptions of webscraping HTML, cleaning HTML, removing unnecessary text, and creation of token count table for application in each of the machine learning models to determine the NSD and DA statuses for each paper. Once the frequency count tables were prepared for each paper, a z-score was applied using the saved data from each model appropriately, using the formula XX $((Observed token frequency - Model token frequency \mu)/(model token frequency sd))$. This z-scoring formula was applied to standardize the frequency of each token. Only tokens included in the machine learning models were retained in experimental datasets. Finally, each model was deployed on each paper to determine its NSD and DA status.

### Statistical Methodology

After each random forest model was deployed on our experimental dataset, we used a negative binomial regression model using the R package MASS (@MASSref). A negative binomial regression model allows us to investigate data where group means are different than the overall dataset mean. A log transformation of the time variable(age.in.months) was applied prior to estimating the statistical regression model to correct for the nature of time as compared to other variables in the dataset. After applying the negative binomial model, we calculated ratios of the estimated number of citations per paper over time by DA status using R package emmeans (@emmeans). We also used R package sjPlot to estimate the 95% CI for estimated citations over time for each journal by DA status (@sjPlotref).

-   Supplemental Material file list (where applicable)
-   Acknowledgments
    -   ***“This research was supported in part through computational resources and services provided by Advanced Research Computing at the University of Michigan, Ann Arbor.”***

        [ARC's RRID is: SCR_027337](https://rrid.site/data/record/nlx_144509-1/SCR_027337/resolver?q=SCR_027337%2A&l=SCR_027337%2A&i=rrid:scr_027337)
-   References
-   Figures/Tables/stats to make/get
    -   table of conditions to add to the methods of classification?