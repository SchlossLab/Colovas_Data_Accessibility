---
title: "Data Accessibility Paper"

editor: visual

date: last-modified
date-format: medium
prefer-html: true

format:
  pdf:
    documentclass: article
    keep-tex: true
    lof: false
    toc: false
    number-sections: false
    output-file: asm_manuscript.pdf
    geometry: margin=1.0in
fontsize: 11pt
linestretch: 1.75
header-includes:
 - \usepackage[left]{lineno}
 - \linenumbers
 - \modulolinenumbers
 - \usepackage{helvet}
 - \renewcommand*\familydefault{\sfdefault}
 - \usepackage[T1]{fontenc}
execute:
  echo: FALSE
  tidy: TRUE
  eval: TRUE
  warning: FALSE
  message: FALSE
  cache: FALSE
runningtitle: 
runningauthor: Colovas

author:
  - name: Joanna Colovas
    orcid: 0009-0005-9180-442X
    affiliations:
      - ref: micro
  - name: Adena Collens
    affiliations:
      - ref: micro
  - name: Patrick D. Schloss
    orcid: 0000-0002-6935-4275
    email: pschloss@umich.edu
    corresponding: true
    affiliations:
      - ref: micro
      - ref: ccmb
affiliations:
  - id: micro
    name: Department of Microbiology & Immunology, University of Michigan
  - id: ccmb
    name: Center for Computational Medicine and Bioinformatics, University of Michigan
abstract: |
 
#importance: TODO
keywords: data accessibility, data reproducibility, supervised machine learning

bibliography: references.bib
csl: asm.csl
---

-   Abstract

-   Importance

    -   Incentivize authors to publish/make available their original data
    -   Publishing data helps get more use out of research
    -   Helps eliminate file drawer effect as it shows negative data

-   Keywords

    -   Data accessibility
    -   Data reproducibility

```{r}
#load data

#library statements
library(tidyverse)

training_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/new_groundtruth.csv")

all_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/predictions_with_metadata.csv.gz")

n_digits <- 4

# #nsd yes metadata needs to be updated at some point
# nsd_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/nsd_yes_metadata.csv.gz")


```

## Introduction

Data availability (DA) is the practice of making raw experimental data and analyses publicly accessible, often via upload in maintained databases. DA is a newly emerging and yet deeply important component of the scientific process in the digital age. With the latest and greatest methodologies available across fields, increasing amounts of data are being generated each day, especially in the biological sciences. Availability of these large quantities of study data and metadata (data about data) is a necessary resource for appropriate use and re-use of data and protocols as well as the recreation of analyses. We believe that policies of data "available on request" are not sufficient to be considered available data. One example was published in *Microbiome*, that a reader in search of data may email the corresponding author, with varying results (@langille_available_2018). We believe that this is simply unacceptable, and set out to determine rates of published raw data in the biological sciences, specifically microbiology, by examining the American Society for Microbiology's (ASM) library of published primary research journals.

### Scientific Data as a Public Good

The United States Government spent over two hundred million dollars (USD) in 2024 on research expenditures (@congress2024). The result of all of these investments are data, paid for in part by taxpayers. Therefore, data is a public good, and best used as a benefit to those who provided the funds for it. Once data has been generated, it can be used not only for initial analyses, but over and over again in future studies or meta analyses. Additionally, data can be used to eliminate possible solutions to a problem by the publishing of "negative data." Thinking of data as a public good, if negative data is published, it can help researchers avoid sinking time and financial resources into the investigation of non-viable hypotheses. This lack of publication of non-fruitful investigation is more commonly known as the "file drawer effect." Data as a public good also is subject to the tragedy of the commons. If no one contributes to the public "commons", data that is available for public use, how can we advance our understanding of our study systems?

### Current DA Policies

Current data availability guidelines have been informed by a number of policies created by funding agencies, peer-review journals, conference and special task groups, as well as community interest groups. In 2011, after the Future of Research Communication (FoRC) conference in Germany to establish FORCE11, a community interest group which seeks to encourage and promote data availability standards. Also in 2011, the Genomic Standards Consortium (GSC) published the MIMARKS/MIxS standards in *Nature Biotechnology* to promote the publication of the "minimum information about a marker gene sequence" (MIMARKS) or "minimum information about x sequence" (MIxS) (@yilmaz_minimum_2011). These standards are checklists usable by data generators and uploaders towards inclusion of relevant data with sequence uploads in the International Nucleotide Sequence Database Collaboration (INSDC).

In 2014 the group published the Joint Declaration of Data Citation Principles (JDDCP), a document towards the standardization of data citation and its future availability(@jddcp2014). The Findable, Accessible, Interoperable, and Reuseable (FAIR) data science guiding principles were put forth in 2016 by Wilkinson et al in *Nature Scientific Data* urges readers to "improve the infrastructure supporting the reuse of scholarly data" (@wilkinson_fair_2016). Neither the JDDCP nor the FAIR principles are enforcable by any agency. Finally, the National Institutes of Health (NIH) began enforcing the "Policy for Data Management and Sharing" (NOT-OD-21-013) in January of 2023, requiring NIH funded studies to submit a data management and sharing plan (DMS) with their funding applications, and comply with their DMS plan after generation and publication of the funded work(@NIH2023). Non-compliance with NOT-OD-21-013 is identified by funding agencies during annual Research Performance Progress Reports (RPPRs), and may impact future funding decisions (@NIH2023).

### ASM Journals

With the advent of next generation sequencing, microbiology research has generated large amounts of sequencing data, and it is common to upload sequence data to a public repository as well as to include data in research publications. The American Society for Microbiology(ASM) is the major professional body recognized by microbiologists. They have eighteen journals, thirteen primary research journals, three review journals, and two archive journals. In addition, several journals have been folded into others or renamed over time. The ASM family of journals requires that authors "make data fully available, without restriction, except in rare circumstances" (@ASMopendatapolicy). They have adapted this policy from journals *Microbial Genomics* and *PLOS*. In the ASM open data policy they describe the use of a "Data Availability Statement" which includes "data description, name(s) of the repositories, and digital object identifiers (DOIs) or accession numbers" and encourages publishing data on relevant public repositories (@ASMopendatapolicy). Consequences of non-compliance to the ASM open data policy include contacting research article authors to inform of non-compliance, publication of an "Expression of Concern" for the author and their compliance issues, sanctions on publication in ASM journals, as well as contacting the affiliated research institution and/or funding agencies of the authors (@ASMcompliance). We endeavor to evaluate how well the microbiology community is using reproducible data practices as we believe that this group of researchers will be early adopters of the technologies available as a result of both the ASM and NIH policies towards data availability.

### Nucleic Acid Sequencing Efforts

Beginning in 1996 with the International Strategy Meeting on Human Genome Sequencing in Bermuda, researchers have prioritized the release of all human genome sequencing information so that it may "maximize its benefit to society" (\@bermuda1996). The meeting participants agreed that "primary sequence data should be rapidly released", with "sequence assemblies \[to\] be released as soon as possible, in some centres, assemblies of greater than 1 kb would be released automatically on a daily basis", and that "finished annotated sequence should be submitted immediately to public databases" (\@bermuda1996). The "Bermuda Principles," as they became known, have been embraced by the large scale Human Genome Project (HGP) since 1998. In 2003, another meeting, held in Ft. Lauderdale, FL, re-affirmed the 1996 Bermuda Principles, expanded upon them to apply more broadly towards sequencing data, and called for further support of these practices (\@ftlauderdale). These foundation agreements set the stage for both the HGP and the Human Microbiome Project (HMP) to generate and share massive amounts of data over the course of their studies (\@HGP/HMP source needed).

Starting with projects such as the HGP and HMP, nucleic acid sequencing efforts have been commonly uploaded and released using public databases. There are three major databases worldwide to support sequencing and sharing efforts. The National Library of Medicine's (NLM) National Center for Biotechnology (NCBI) in the United States, the Research Organization of Information Systems' (ROIS) National Institute of Genetics (NIG) in Japan, and the European Molecular Biology Lab's (EMBL) European Bioinformatics Institue (EBI) in Europe. These three databases are part of the International Nucleotide Sequence Database Collaboration (INSDC). These large databases make research by comparison possible. Genetic lineages of microbes are determined by creating phylogenetic trees which compare a new sequence to existing sequences. Phylogenetic trees show how closely related a new microbial genetic sequence is related to others studied before both in terms of evolution and mutation and in structure and function. An important tool for creating phylogenies is the NCBI Basic Local Alignment Search Tool (BLAST) (@altschul1990). The BLAST algorithm allows users to compare a nucleic acid or protein sequence to the NCBI database of over 1TB of data to find similar and related sequences. Without the upload of sequences to the NCBI database, the use and success of BLAST would not be possible, despite the effort required on part of the researcher to upload of sequences to one of the INSDC databases.

### Examples of Data Avalability

A key tenet of the scientific method is the ability to replicate scientific findings to ensure that they are not due to error. One way that scientific findings can be replicated is by re-completing the same analyses by another researcher. This is only possible if the data used to complete the original analyses is available for use. The availability of datasets also allows new questions to be answered with existing data or the combination of multiple datasets, such as the use of the Human Microbiome Project's (HMP) sequencing data by researchers to create over 650 scientific publications (@HMP), and the completion of metadata studies. Availablity of data contributed to the rapid sequencing of the SARS-CoV-2 virus during the 2020 pandemic and subsequent expedition of vaccine development (@needCOVIDref).

With microbiologists commonly uploading nucleic acid sequences to public databases, the aim of this study was to determine the current state of data availability in twelve primary research journals from the ASM family of journals. Primary research articles were classified with using two machine learning models to answer two questions; "Does this paper contain new sequence data?", and "Is the data available?" Once these questions were answered, we moved to analyses to answer further questions, "How does making my data available impact my citation metrics over time?"

## Results

### General Description of the Experiment

We set out to determine the current state of data availability in twelve primary research journals from the ASM family of journals. This objective was completed by first acquiring all papers published in the journals of interest between 2000 and 2024 using the Crossref database and command line tools. We then trained random forest machine learning models to differentiate if each paper contained "New Sequencing Data" (NSD) and then if the paper had "Data Available" (DA). To avoid overfitting the model, we trained each model multiple times, performing validations on a subset of data after each iteration. This allowed us to have a greater number of papers in the training dataset, as well as to have great accuracy and precision within our models. Using our trained models, we were able to classify over 150,000 papers from the whole dataset to determine if they were NSD or DA. After this, we could perform statistical modeling to describe the data, and generate summary statistics. We were especially interested in the ways in which NSD and DA impact citation metrics.

### ASM Journals

We used twelve of the ASM primary research journals in this study. Of note, several journals had changes to their publication goals during the 2000-2024 time period. The *Journal of Bacteriology* was the primary place to publish new genome announcements until 2013 when ASM announced journal *Genome Announcements* as a more permanent place for this type of data. *Genome Announcements* was active from 2013 until 2018, when it was re-branded to *Microbiology Resource Announcements*, which has been active from 2018 until present. These two journals appear separately in our analyses as a result of the Crossref database. New genome announcements have a high percentage of NSD papers, and a high percentage of DA within those papers. As a result, the *Journal of Bacteriology* has had fewer NSD papers, and fewer papers with DA since 2013. Another journal of note is *Microbiology Spectrum* and its re-brand. From 2013 until the fall of 2021, *Microbiology Spectrum* was a review journal. After this point, *Microbiology Spectrum* became a primary research journal. Review journals are less likely to publish articles with NSD, and to have DA. Several journals, including *Microbiology Spectrum*, do not span the entire time period for the study. Journals *mBio* (b.2010), *Microbiology Spectrum* (b. 2013, re-brand 2021), *mSphere* (b. 2016), *mSystems* (b. 2016), and *Genome Announcements* (2013-2018). Not all journals are equally likely to contain NSD and have sequencing DA as a result of their field of interest. Journals *Antimicrobial Agents and Chemotherapy; Infection and Immunity;* and the *Journal of Microbiology and Biology Education;* are all less likely to contain NSD and have DA than the other journals in the dataset.

### Descriptive Statistics:

```{r make_data_tables}

training_data_table <-
training_data %>% 
    count(container.title, new_seq_data, data_availability) %>%
    complete(., container.title, new_seq_data, data_availability, fill = list(`n` = 0L)) %>%
    mutate(.by = container.title, 
               n_total = sum(`n`), 
           n_fract = (n_total/sum(.$`n`)*100))  %>% 
    filter(new_seq_data == "Yes")  %>%
       mutate(.by = container.title,
           n_nsd = sum(`n`), 
           fract_nsd = n_nsd/n_total*100) %>%
    filter(data_availability == "Yes") %>% 
    mutate(.by = container.title, 
          n_da = `n`, 
          fract_da = n_da/n_nsd*100) %>%
    select(container.title, n_total:fract_da) # i think the NaN is fine for now

kableExtra::kable(training_data_table, caption = "Summary Statistics of Model Training Dataset", 
                digits = n_digits)

 
#whole dataset yay!
whole_dataset_table <-
all_data %>% 
    count(container.title, nsd, da) %>%
    na.omit() %>%
    complete(., container.title, nsd, da, fill = list(`n` = 0L)) %>%
    mutate(.by = container.title, 
               n_total = sum(`n`), 
           n_fract = (n_total/sum(.$`n`)*100))  %>% 
    filter(nsd == "Yes")  %>%
       mutate(.by = container.title,
           n_nsd = sum(`n`), 
           fract_nsd = n_nsd/n_total*100) %>%
    filter(da == "Yes") %>% 
    mutate(.by = container.title, 
          n_da = `n`, 
          fract_da = n_da/n_nsd*100) %>%
    select(container.title, n_total:fract_da)

kableExtra::kable(whole_dataset_table, caption = "Summary Statstics of All Data", 
                digits = n_digits)




```

```{r whole_dataset_stats}
#whole dataset



## whole dataset stats
n_total <- nrow(all_data)
all_per_nsd <- count(all_data, nsd) %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 3]]

all_per_da <- count(all_data, nsd, da) %>%
            filter(nsd == "Yes") %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 4]]


#year distribution - do we want a figure for this or the table? 
# do we want to combine this with the training set data? 
all_years <- 
   all_data %>% 
    count(., year.published) %>%
    rename(., n_whole_dataset = `n`)



#journal w/ highest and lowest rate of nsd
all_fract_high_nsd <- max(whole_dataset_table$fract_nsd)
all_j_high_nsd <- whole_dataset_table[[which.max(whole_dataset_table$fract_nsd), 1]]
all_fract_low_nsd <- min(whole_dataset_table$fract_nsd)
all_j_low_nsd <- whole_dataset_table[[which.min(whole_dataset_table$fract_nsd), 1]]

#journals w/ high/low rate da - two journals have 100% DA
all_fract_high_da <- max(whole_dataset_table$fract_da, na.rm = TRUE)
all_j_high_da <- whole_dataset_table[whole_dataset_table$fract_da == all_fract_high_da, 1] %>%
               stringr::str_flatten(, collapse = " and ", na.rm = TRUE)

all_fract_low_da <- min(whole_dataset_table$fract_da, na.rm = TRUE)
all_j_low_da <- whole_dataset_table[[which.min(whole_dataset_table$fract_da), 1]]


## avg citations/paper
all_total_avg_cites <- all_data %>% 
    select(file, container.title, is.referenced.by.count) %>% 
    summarize(container.title = "Total",  
              mean_refs = mean(is.referenced.by.count, na.rm = TRUE), 
               median_refs = median(is.referenced.by.count, na.rm = TRUE))

all_avg_cites_j <- all_data %>% 
    select(file, container.title, is.referenced.by.count) %>% 
    summarize(.by = container.title, 
              mean_refs = mean(is.referenced.by.count, na.rm = TRUE), 
               median_refs = median(is.referenced.by.count, na.rm = TRUE))
all_avg_cites <- rbind(all_avg_cites_j, all_total_avg_cites)

all_max_median_cites <- max(all_avg_cites_j$median_refs, na.rm = TRUE)
all_j_max_median_cites <- all_avg_cites_j[[which.max(all_avg_cites_j$median_refs), 1]]

all_min_median_cites <- min(all_avg_cites_j$median_refs, na.rm = TRUE)
all_j_min_median_cites <- all_avg_cites_j[[which.min(all_avg_cites_j$median_refs), 1]]
```

#### Whole Dataset

Using the Crossref database, with validation from WOS, NCBI, and Scopus databases, we downloaded N = `r n_total` unique records of papers published in ASM journals between 2000 and 2024. After downloading the HTML content of each paper, we cleaned the HTML content and readied it to apply our machine learning models to classify each paper. Overall, `r all_per_nsd`% of papers had NSD, and `r all_per_da`% of papers with NSD, had DA. See table 1 for percentages of NSD and DA for each journal. The journal with the highest rate of NSD was `r all_j_high_nsd` at `r all_fract_high_nsd`%, and the lowest was `r all_j_low_nsd` at `r all_fract_low_nsd`%. The journal with the highest rate of DA was `r all_j_high_da` at `r all_fract_high_da`%, and the lowest was `r all_j_low_da` at `r all_fract_low_da`%. This was expected as `r all_j_high_nsd` publishes mainly new genomic sequence data and makes the data available. On average, papers in the dataset had a median of `r all_total_avg_cites$median_refs` citations/article. This number varies by journal, see table YYYY for data by journal. The journal with the highest median rate of citations/article was `r all_j_max_median_cites` at `r all_max_median_cites`%, and the lowest was `r all_j_min_median_cites` at `r all_min_median_cites`%. The journals in the dataset span years 2000-2024. See table YYYYY for the distribution of papers per year in the dataset.

```{r training_data_stats}
#ok now we need to find the percentages/data that we need 

## Training dataset stats

n_training <- nrow(training_data)
t_per_nsd <- count(training_data, new_seq_data) %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 3]]

t_per_da <- count(training_data, new_seq_data, data_availability) %>%
            filter(new_seq_data == "Yes") %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 4]]

#year distribution - do we want a figure for this or the table? 
t_years <- 
    training_data %>% 
    count(., year.published) %>%
    rename(., n_training_dataset = `n`)

year_pub_table <-
full_join(all_years, t_years, by = join_by(year.published)) %>%
    kableExtra::kable(., caption = "Distribution of Year Published for Whole and Training Dataset", 
                        digits = n_digits)

#journal w/ highest and lowest rate of nsd
t_fract_high_nsd <- max(training_data_table$fract_nsd)
t_j_high_nsd <- training_data_table[[which.max(training_data_table$fract_nsd), 1]]
t_fract_low_nsd <- min(training_data_table$fract_nsd)
t_j_low_nsd <- training_data_table[[which.min(training_data_table$fract_nsd), 1]]

#journals w/ high/low rate da - two journals have 100% DA
t_fract_high_da <- max(training_data_table$fract_da, na.rm = TRUE)
t_j_high_da <- training_data_table[training_data_table$fract_da == t_fract_high_da, 1] %>%
               stringr::str_flatten(, collapse = " and ", na.rm = TRUE)

t_fract_low_da <- min(training_data_table$fract_da, na.rm = TRUE)
t_j_low_da <- training_data_table[[which.min(training_data_table$fract_da), 1]]


## avg citations/paper
t_total_avg_cites <- training_data %>% 
    select(paper, container.title, is.referenced.by.count) %>% 
    summarize(container.title = "Total",  
              mean_refs = mean(is.referenced.by.count), 
               median_refs = median(is.referenced.by.count))

t_avg_cites_j <- training_data %>% 
    select(paper, container.title, is.referenced.by.count) %>% 
    summarize(.by = container.title, 
              mean_refs = mean(is.referenced.by.count), 
               median_refs = median(is.referenced.by.count))
t_avg_cites <- rbind(t_avg_cites_j, t_total_avg_cites)

t_max_median_cites <- max(t_avg_cites_j$median_refs, na.rm = TRUE)
t_j_max_median_cites <- t_avg_cites_j[[which.max(t_avg_cites_j$median_refs), 1]]

t_min_median_cites <- min(t_avg_cites_j$median_refs, na.rm = TRUE)
t_j_min_median_cites <- t_avg_cites_j[[which.min(t_avg_cites_j$median_refs), 1]]

```

#### Training Dataset **(pretty much the same as the above paragraph, but with stats for the training dataset)**

\*XX - add italics for journal names even when they're variables, do this in not visual mode

We created a subset of the whole dataset to train our machine learning models. The training dataset initially had N = 500 papers, but was increased over time due to gaps in the dataset, and after subsequent validation of the trained models (see below), with a total of N = `r n_training`. See table 2 for the distribution of papers per journal in the training dataset. The journals in the dataset also span years 2000-2024. See table 3 for the distribution of papers per year in the training dataset. Overall, `r t_per_nsd`% of papers had NSD, and `r t_per_da`% of papers with NSD, had DA. See table 2 for percentages of NSD and DA for each journal. The journal with the highest rate of NSD was `r t_j_high_nsd` at `r t_fract_high_nsd`%, and the lowest was `r t_j_low_nsd` at `r t_fract_low_nsd`%. The journals with the highest rate of DA in NSD papers were `r t_j_high_da` at `r t_fract_high_da`%, and the lowest was `r t_j_low_da` at `r t_fract_low_da`%. On average, papers in the dataset had median `r t_total_avg_cites$median_refs` citations/article. This number varies by journal, see table YYYY for data by journal. The journal with the highest rate of citations/article was `r t_j_max_median_cites` at `r t_max_median_cites`%, and the lowest was `r t_j_min_median_cites` at `r t_min_median_cites`%.

### Year Published Distribution Table

`r year_pub_table`

### Descriptive Statisitcs about the Trained Models

```{r trained_model_stats}
best_models <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/best_model_stats.csv")

kableExtra::kable(best_models, caption = "Trained Model Summary Statistics", 
                    digits = n_digits)

```

#### Figures for trained models 
![](../Figures/ml_results/groundtruth/rf/auroc.new_seq_data.png)
![](../Figures/ml_results/groundtruth/rf/auroc.data_availability.png)
!["Mtry Performance New Sequencing Data"](../Figures/ml_results/groundtruth/rf/hp_perf.rf.new_seq_data.png)
!["Mtry Performance Data Availability"](../Figures/ml_results/groundtruth/rf/hp_perf.rf.data_availability.png)




Two random forest models were trained to predict if published scientific papers "contained new sequence data" (NSD), and if the paper "had data available" (DA), one model for each variable. Other models such as generalized linear regression and boosted trees were explored, but were ultimately discarded in favor of the random forest model (data not shown). Random forest models were chosen to aid in this classification problem as the creation of many decision trees helps to improve accuracy and precision. This type of model has one hyperparameter, 'mtry' or the number of predictors to be sampled at each decision. During iterative model training, a subset of papers were validated after each completed training and deployment of each model. Papers from each journal, and extras from certain journals were hand-validated against model predictions to generate confusion matrices. Confusion matrices for the final version of each trained model are available in YYYY table(supplement?). The NSD model used an mtry value of `r best_models[[1, 3]]` had an Area Under the Curve(AUC) of `r best_models[[3, 3]]` and an accuracy of `r best_models[[5, 3]]`. The sensitivity of the NSD model was `r best_models[[8,3]]`, and the specificity of the model was `r best_models[[9,3]]`. The DA model used an mtry value of `r best_models[1,2]` had an AUC of `r best_models[[3,2]]` and an accuracy of `r best_models[[5,2]]`. The sensitivity of the DA model was `r best_models[[8,2]]`, and the specificity of the model was `r best_models[[9,2]]` (See table 4 for more information on trained machine learning models). This shows that the models fit the data well, and can provide classifications on new data with an expected error rate of less than 10%. We deemed this as acceptable, accounting for variability in papers and data, as well as the large size of the dataset on which we deployed the models.

### Regression Model using Negative Binomial Models

```{r neg_binomial}

neg_bin <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/negative_binomial/nsd_yes_glmnb_coeftable.csv")

neg_bin_table <- 
   kableExtra::kable(neg_bin, caption = "Coefficients for Negative Binomial Modeling", 
    digits = n_digits) 

```

`r neg_bin_table`

In this study we sought to investigate the effect of NSD and DA on the number of citations received by a given paper. We focused on NSD papers to determine the effect of having DA. This led us to the use of a negative binomial regression model to best describe our data. All regression data was NSD yes. We focused on the continuous outcome of "number of citations" with predictor variables journal (categorical), age in months (continuous), and DA status (dichotomous). Due to the number of citations being bell shaped with a long right tail (very few papers at advanced age with many citations), the model that best described our data was the negative binomial regression model. A negative binomial model is appropriate for data that begins at zero and has a long 'tail' of data. This model also includes a dispersion parameter to describe the spread of the data. We applied a log transformation to our age variable (age.in.months) to better describe the relationship between time and number of citations received. See table XXX6? for model coefficients. In general, we found that NSD papers that made DA received more citations over time than those that did not. See figure YYYY for trends in each major journal. ??XX insert some kind of metrics about % more citations over time? per journal? ??XX Describe figure emmeans contrast plot that i cannot get to insert in the document for the life of me??? In all journals excepting the *Journal of Bacteriology*, papers with DA have a greater number of citations at time point 60 months after publication, if not sooner via ratio plot. Over time this ratio increases further, demonstrating increased citations for papers with DA available over time.

#### Figures for Negative Binomial yay! come back to theseXX

!["Ratio of DA Yes to DA No"](../Figures/negative_binomial/emmeans_contrast_plot.png)
![](../Figures/negative_binomial/model_predicted_plot.png)


## Discussion

We investigated the impact of data availability on citation metrics in new sequencing papers by deploying machine learning models on over 150,000 papers from the ASM family of journals. Overall, making data available increases citation metrics over time.

On average, more than half of papers with NSD had DA (`r all_per_da`%), showing that authors of NSD papers are more often than not, making their data publicly available. This is in line with recent NIH policy requiring that data be made available using the XXdata planXX outlined in the NIH's NOT-OD-21-013 @noauthor_not-od-21-013_nodate. Expectledly, journals *Genome Announcements* and *Microbiology Resource Announcements* had the highest rates of DA in NSD papers. These journals publish primarily new genomic sequences and are required by ASM to make data available. Journals such as the *Journal of Microbiology and Biology Education* and *Infection and Immunity* which publish fewer NSD papers, have lower rates of DA in their journals.

Next, we looked further into the impact of DA on citation metrics using a negative binomial regression model. Using this model we found that over time, papers with DA receive more citations than those without up to well over 1.5x in some journals (*Journal of Clinical Microbiology* ). This effect intensifies over time, with the greatest differences in citations occuring at the 108 months since publication time point. This is great news to manuscript authors, that simply making DA can give as much as 50% increase in citations over time. We believe that this more than justifies the work of making data available. We hope that these data will help to incentivize authors to make data available.

We acknowledge the limitations of our study data, focusing only on papers published in the ASM family of journals. We understand that this relationship between DA and citation metrics may not be as strong in other families of journals, but we hope with the newest NIH funding policies, these trends will continue. Another limitation is the availability of paper metadata from various databases, with crossref's metadata having the most complete metadata available for each paper. We were also not able to see specific time points, only having access to citation data for papers at time of dataset download (February 2025), and no intermediate time points for any given paper.

While DA in publications has a citation advantage, we hope that is not the only reason authors choose to make their data available. The need for reproducibility in science and demand for large datasets are additional reasons. We hope that authors recognize the need to contribute to the data "commons", to further work done by others, and that even their negative results have value and power to stop others from continuing down dead ends.

## Materials and Methods

### Creation of the Training set

To train our random forest machine learning model, we first created an appropriate training data set. Using the crossref database, we first downloaded all papers from the selected ASM family of journals from the time period beginning January 1st, 2000, and ending on December 31st, 2024. The data was updated as of February 10th, 2025 with all citation counts frozen at that date. For our initial training set, we chose N = 500 papers from across each journal and time period, adding special emphasis to include papers that were part of our desired set of interest (i.e. contained published data) to ensure that our two models could adequately characterize each paper as a new sequencing paper and if it published raw sequencing data or not. After creating our initial dataset, it was necessary to identify the status of both variables by hand and determine if each paper contained "new sequencing data" (NSD), and if each one had "data available" (DA). This was completed by opening each paper in an internet browser window, and searching for a "data availability" or similar statement. See table XXX for specific cases and how each of these cases were identified for the purpose of this study.

| Scenario | NSD Status | DA Status |
|----------------------------------------------|-------------|-------------|
| Paper is not about generating new sequencing data | No | No |
| Paper is about generating new sequencing data but has no data available | Yes | No |
| Paper is about generating new sequencing data and has data available | Yes | Yes |
| Paper uses sequencing as a confirmation of experimental technique (i.e. confirmation of plasmid insertion) | No | No |
| Paper discusses new computational or experimental tools | No | No |
| Paper has microarray data | No | No |
| Papers using MLST ONLY | No | No |
| Papers using qPCR ONLY | No | No |
| Papers about protein sequencing that have nucleotide sequencing | Yes | Yes/No depending on DA |
| Papers using iRNA techniques | No | No |
| Papers using pyrosequencing/454 techniques | Yes | Yes/No depending on DA |

: Possible Data Scenarios

### Adding Additional Training Set Papers

After initial trainings of our random forest models, a random sampling of papers was collected for each journal to audit the efficacy of the models. To audit the efficacy of the models, we hand identified the status of both variables of interest, NSD and DA. We looked for weaknesses in the models, and updated methodology to reflect important areas of interest. For example, in 2023 the ASM journals changed their formatting to include the data availability statement of a paper in a sidebar of the webpage. We identified this by noticing that all papers from journal *Microbiology Resource Announcements* from 2023-2024 were incorrectly characterized by the model as DA = No. The sidebar of the webpage was not included in the text the model was considering, and code had to be updated to include all sidebar data for all papers. These improvements to the model created a larger and more comprehensive training set of N = 9XX. These validations allowed us to create confusion matricies for each model. Confusion matricies for the final version of each trained model are available in YYYY table(supplement?).

### Descriptive Statistics about the Training Set

*I think this is covered earlier but i will leave it for now just to make sure that i can move the sentences around if i need to*

There were XXX papers in the training data set from 12 ASM journals. These papers came from journals *Applied and Envrionmental Microbiology; Antimicrobial Agents and Chemotherapy; Infection and Immunity; Journal of Clinical Biology; Journal of Virology; Journal of Bacteriology; Journal of Microbiology and Biology Education; Microbiology Resource Announcments* (formerly known as *Genome Announcements*); *mSystems; mSphere; mBio; and Microbiology Spectrum.* See table XXXX for the number of papers from each journal in the training dataset. The training dataset includes journal articles published between January 1st, 2000 and December 31st, 2024. See table XXXXX for the number of papers included from each year from 2000-2024. XXX% of training set data was NSD = Yes, and XXX% of training set data was DA = Yes.

### Creation of the Training Data from Training dataset

To perform the computational steps required for these experiments, we used the python tool Snakemake (@snakeref), and the University of Michigan's high performance computing cluster (@arc ref). Using our selected papers from the training dataset, we downloaded the entirety of each paper's source HTML using the command line tool wget. This allowed us to use the source HTML multiple times for updated analyses without the need to re-query the ASM webservers numerous times. Next, we performed cleaning of the HTML using R packages rvest (@rvest ref) and xml2 (@xml2 ref) to get the desired portions of the paper from the HTML including the abstract, the body of paper, all tables and figureswith captions, as well as the side panels for all papers, but especially those containing the data availability statements in papers published after the 2023 change in webpage format (see above). Then we removed unnecessary text using R packages tm(text manipulation)(@tm ref) and textstem (@textstem ref), as well as converting all text to lowercase, and the removal of digits and non-alphabetic characters such as whitespace. To have the fewest number of unique words, we lemmatized (sort words by grouping inflected or variant forms of the same word) words to trace them back to their root words and eliminate any possible issues with word tense. After this, we created and counted our 'tokens', phrases of up to 1-3 consecutive words from the text of the paper using R package tokeinziers (@tokeinziers ref). Towards the goal of the fewest meaningful number of words, we used the 'Snowball' (@snowball ref) dictionary of 'stop words' to remove non-meaningful words such as articles 'a', 'an', and 'the'. We removed the 'space' character with an underscore in multi-word tokens for ease of procesing, and created a count table for the tokens in each paper.

Once the tokens in each paper were counted, we transformed the data into a sparse matrix format useable by the R package mikropml (@mikropml ref), using R packages caret and dplyr (@caret ref, @dplyr ref). Tokens were filtered to those which appear in greater than one paper. This allows comparison between papers by the model. We removed near zero variants (tokens with frequency very close to zero) as well as collapsing perfectly correlated tokens (tokens that always appear together) using R packages caret and mikropml to reduce model complexity. The data was then simplified to keep only the following variables; tokens, frequency, journal information, and hand identified NSD and DA variables. This simiplified sparse matrix data had the mean and standard deviation calculcated and saved for the frequency of each token to later apply a z-scoring method to future data to be predicted by the model.

### Training of the DA and NSD Models

We trained two random forest machine learning models using mikropml's "run_ml" function, one to determine if a paper contained new sequence data (NSD), and another to determine if the paper had data availabile (DA). The mikropml "run_ml" function uses methodology described by Topcuoglu et al (@topcuoglu2020) to split data for model training. Random forest models have one hyperparameter to tune, the mtry value. We began with mtry values of 100, 200, 300, 400, 500, and 600, to find peak hyperparameter performance given *N tokens*. We trained the models multiple times in accordance with existing methologies, first to find the optimal Area Under the Receiver-Operator Curve (AUROC) value for each model with N=100 seeds. Then to find the best mtry performance for each model, with N=1 seed. Finally, with N=1 seed to train each final model for use on experimental data.

### Preparation of the Experimental Dataset

To fully answer our research questions, we created a larger database with N = 155779 papers curated from reference datasets Crossref, NCBI, Scopus, and the Web of Science (@crossref, @ncbi, @scopus, @wos). These papers span all twelve ASM journals of interest from the start of 2000 to the end of 2024. Once database was curated, we applied the same steps to ready papers for application of machine learning models as the model training datasets. See above for descriptions of webscraping html, cleaning html, removing unnecessary text, and creation of token count table for application in each of the machine learning models to determine the NSD and DA statuses for each paper. Once the frequency count tables were prepared for each paper, a z-score was applied using the saved data from each model appropriately, using the formula XXX((observed_token_frequency - model_token_frequency_mean)/(model_token_frequency_sd)). This z-scoring formula was applied to standardize the frequency of each token. Only tokens included in the machine learning models were retained in experimental datasets. Finally, each model was deployed on each paper to determine its NSD and DA status.

### Statistical Methodology

After each random forest model was deployed on our experimental dataset, we used a negative binomial regression model using the R package MASS (\@MASSref). A negative binomial regression model allows us to investigate data where group means are different than the overall dataset mean. A log transformation of the time variable(age.in.months) was applied prior to estimating the statistical regression model to correct for the nature of time as compared to other variables in the dataset. After applying the negative binomial model, we calculated ratios of the estimated number of citations per paper over time by DA status using R package emmeans (\@emmeansref). We also used R package sjPlot to estimate the 95% CI for estimated citations over time for each journal by DA status (\@sjPlotref).

-   Supplemental Material file list (where applicable)
-   Acknowledgments
-   References
-   Figures/Tables/stats to make/get
    -   table of conditions to add to the methods of classification?