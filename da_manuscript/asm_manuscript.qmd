---
title: "Data Accessibility Paper"
author: "Joanna Colovas"
format: pdf
editor: visual
---

-   Abstract
-   Importance
    -   Incentivize authors to publish/make available their original
        data
    -   Publishing data helps get more use out of research
    -   Helps eliminate file drawer effect as it shows negative data
-   Keywords
    -   Data accessibility
    -   Data reproducibility
-   Introduction
    - Scientific Data as a Public Good
        -   Data is generated by public funding
        -   Negative results can help avoid research costs aka filedrawer effect
        -   The tragedy of the commons
    -   Why make data available? 
        -   Replication of studies
        -   Ability to compare results across conditions
        -   Ability to perform metadata studies 
        -   Ability to have a wide variety of analyses
            -   See human microbiome project data and publications (HMP)
        -   Examples of data availability helping people out
            -   COVID sequencing and data availability was essential to vaccine
            development
            -   Comparison of sequencing across conditions for not so popular model organisms?
                -   BLAST - does BLAST use everything in NCBI? 
                -   Tragedy of the commons? if no one uploads to NCBI then there's nothing to search and the tools are basically useless?


    Scientific Data as a Public Good
    The United States Government spent over two hundred million dollars (USD) in 2024 on research expenditures (@congress.gov2024). The result of all of these investments are data, paid for in part by taxpayers. Therefore, data is a public good, and best used as a benefit to those who provided the funds for it. Once data has been generated, it can be used not only for initial analyses, but over and over again in future studies or meta analyses. Additionally, data can be used to eliminate possible solutions to a problem by the publishing of "negative data." Thinking of data as a public good, if negative data is published, it can help researchers avoid sinking time and financial resources into the investigation of non-viable hypotheses. This non-fruitful investigation is more commonly known as the "filedrawer effect." Data as a public good also is subject to the tragedy of the commons. If no one contributes to the public "commons", data that is available for public use, how can we advance our understanding of our study systems? 

    Why Make Data Available?
    A key tenent of the scientific method is the ability to replicate sceintific findings to ensure that they are not due to error. One way that scientific findings can be replicated is by re-completing the same analyses by another researcher. This is only possible if the data used to complete the original analyses is available for use. 



    -   Current data availability and reproducibility guidelines
        -   NIH funded research must make data available as of January 2023
        (Policy for Data Management and Sharing (NOT-OD-21-013))
            -   How do they enforce this? 
            -   Do they even enforce this? 
        -   FORCE11 group &  JDCCP standards
        -   FAIR principles
        -   All of these standards are at least 5 years old
        -   None of these are really all that enforceable by any agency 

    Current data availability and reproducibility guidelines have been informed by a number of policies created by funding agencies, peer-review journals, conference and special task groups, as well as community interest groups. In 2011, after the Future of Research Communication (FoRC) conference in Germany to establish FORCE11, a community interest group which seeks to encourage and promote data availability standards. In 2014 the group published the Joint Declaration of Data Citation Principles (JDDCP), a document towards the standardization of data citation and its future avaialability(@Data Citation Synthesis Group 2014). The Findable, Accessible, Interoperable, and Reuseable (FAIR) data science guiding principles were put forth in 2016 by Wilkinson et al in *Nature Scientific Data* urges readers to "improve the infrastructure supporting the reuse of scholarly data" (@Wilkinson 2016). Finally, the National Institutes of Health (NIH) began enforcing the "Policy for Data Management and Sharing" (NOT-OD-21-013) in January of 2023, requiring NIH funded studies to submit a data management and sharing plan (DMS) with their funding applications, and comply with their DMS plan after generation and publication of the funded work(@NIH2023). 
    
    -   DNA sequencing efforts are commonly uploaded to databases
         -   Comparison is really essential in this field
                    -   Phylogenetic trees are made by comparing similarity of sequences
                    -   Especially trying to identify new sequences by fitting the into existing phylogeny
        -   International Nucleotide Sequence Database Collaboration
            (INDSC) databases
            -   ROIS - NGL (Japan)
            -   EMBL - EBI (Europe)
            -   NLM - NCBI (USA)
        -   This is actually more work for the data generator to make sure that their data is uploaded somewhere


    -   ASM journals
        -   ASM is the major professional body of microbiologists
        -   They have 12 journals (list of journals)
        -   Data availability policies of the journal
        -   Enforcement of da policies of the journal
        -   Why are we starting here with these journals? 
            -   Microbiology research generates large amounts of data
            -   It's common to upload them to a secondary database and not just include the data in a publication
             -   Want to evaluate how well this community is using reproducible data analysis as a metric because we think they will be early adopters of the technology

With the advent of next generation sequencing, microbiology reserach has generated large amounts of sequencing data, and it is common to upload sequence data to a public repository as well as to include data in research publications. The American Society for Microbiology(ASM) is the major professional body recognized by microbiologists. They have eighteen journals, thirteen primary research journals, three review journals, and two archive journals. In addition, several journals have been folded into others or renamed over time. The ASM family of journals requires that authors  "make data fully available, without restriction, except in rare circumstances" (@ASM website open data policy). They have adapted this policy from journals *Microbial Genomics* and *PLOS*. In the ASM open data policy they describe the use of a "Data Availability Statement" which includes "data description, name(s) of the repositories, and digital object identifiers (DOIs) or accession numbers" and encourages publishing data on relevant public repositories (@ASM website open data policy). We endeavor to evaluate how well the microbiology community is using reproducible data practices as we believe that this group of researchers will be early adopters of the technologies available. 
    


    -   Microbiology data 
        -   Why do people upload sequences?
        -   Incentives? 

    -   Goals
        -   Investigate metrics of making data publicly available in 12 ASM journals
        -   Using machine learning models 
        -   To see what kind of data availability uptake we have in sequencing fields where there's a known repository
        
-   Results
    -   Models
        -   random forest modeling gave the best results
    -   Model prediction results
        -   AUROC
        -   hyperparameter tuning 
    -   Confusion Matrices for each model
        -   Spot checking and error methodology
    -   Regression modeling
        -   negative binomial regression equations fit the best for this type of data
        -   regression equations
        -   number of citations corrected for the month published
        -   age in months taken log
        -   Regression modeling/confusion matrices for only papers that contain new sequence data
    
-   Discussion
    -   Percent of ASM papers that have new sequence data available (nsd Yes, da Yes)
        -   trends by journal
        -   trends by year
    -   Making data available 
        -   provides more citations per paper than not doing so by *XXX* number
        -   Allows for replication of studies
    -   Why bother doing this?
        -   What advantage does it give you as a data generator 
        -   Is it worth the work? 
    
    -   
-   Materials and Methods
    -   Creation of the training set
        -   Original data set from Adena,
            -   downloaded from crossref when? 2/10/25
                -   did i update the metadata when i re-pulled from crossref
                -   yes i did 
            -   how did she choose these? this is my guess
                -   variety of the 12 journals, years represented
                -   date published from 2000 to 2024
                -   adding of additional papers to cover gaps in representative-ness of datasets as well as the change of format in 2023 (put this after the training of the model)
        -   Must have metadata from crossref
        -   Hand identifying 500 papers for da and nsd status
            -   new_seq_data(nsd) = "Is this paper about new sequencing data that has been generated?"
            -   data_availability(da) = "Did this paper make sequencing data available online such as in NCBI, SRA, EMBL, etc?" 
            -   A paper must be new_seq_data == "Yes" to have data_availability == "Yes".
            -   Papers that use sequencing as a confirmation of experimental technique are nsd = No, da = No
            -   Papers that are about new computational or experimental tools (i.e. mothur) are nsd = No, da = No
            -   Papers using microarray data are nsd = No, da = No
            -   Papers using MLST are not in and of themselves sequencing papers and are nsd = No, da = No
            -   Papers using qPCR only also are nsd = No, da = No 
        -   Training of model for initial testing 
        -   Spot checking initial modeling and realizing there were errors 
            -   do we need to include this?
            -   Adding +150 *(need to check number)* from spot checking exercises to get to N = ***

    Creation of the Training set
    To train our random forest machine learning model, we first created an appropriate training data set. Using the crossref database, we first downloaded all papers from the selected ASM family of journals from the time period beginning January 1st, 2000, and ending on December 31st, 2024. The data was updated as of February 10th, 2025 with all citation counts frozen at that date. For our initial training set, we chose N = 500 papers from across each journal and time period, adding special emphasis to include papers that were part of our desired set of interest (i.e. contained published data) to ensure that our two models could adequately characterize each paper as a new sequencing paper and if it published raw sequencing data or not. After creating our initial dataset, it was necessary to identify the status of both variables by hand and determine if each paper contained "new sequencing data" (NSD), and if each one had "data available" (DA). This was completed by opening each paper in an internet browser window, and searching for a "data availability" or similar statement. See table XXX for specific cases and how each of these cases were identified for the purpose of this study. 

    Adding Additional Training Set Papers
    After initial trainings of our random forest models, a random sampling of papers was collected for each journal to audit the efficacy of the models. To audit the efficacy of the models, we hand identified the status of both variables of interest, NSD and DA. We looked for weaknesses in the models, and updated methodology to reflect important areas of interest. For example, in 2023 the ASM journals changed their formatting to include the data availability statement of a paper in a sidebar of the webpage. We identified this by noticing that all papers from journal *Microbiology Resource Announcements* from 2023-2024 were incorrectly characterized by the model as DA = No. The sidebar of the webpage was not included in the text the model was considering, and code had to be updated to include all sidebar data for all papers. These improvements to the model created a larger and more comprehensive training set of N = 9XX. 
     

    Creation of the Training Data from Training dataset
    To perform the computational steps required for these experiments, we used the python tool Snakemake (@snakeref), and the University of Michigan's high performance computing cluster (@arc ref). Using our selected papers from the training dataset, we downloaded the entirety of each paper's source HTML using the command line tool wget. This allowed us to use the source HTML multiple times for updated analyses without the need to re-query the ASM webservers numerous times. Next, we performed cleaning of the HTML using R packages rvest (@rvest ref) and xml2 (@xml2 ref) to get the desired portions of the paper from the HTML including the abstract, the body of paper, all tables and figureswith captions, as well as the side panels for all papers, but especially those containing the data availability statements in papers published after the 2023 change in webpage format (see above). Then we removed unnecessary text using R packages tm(text manipulation)(@tm ref) and textstem (@textstem ref), as well as converting all text to lowercase, and the removal of digits and non-alphabetic characters such as whitespace. To have the fewest number of unique words, we lemmatized (sort words by grouping inflected or variant forms of the same word) words to trace them back to their root words and eliminate any possible issues with word tense. After this, we created and counted our 'tokens', phrases of up to 1-3 consecutive words from the text of the paper using R package tokeinziers (@tokeinziers ref). Towards the goal of the fewest meaningful number of words, we used the 'Snowball' (@snowball ref) dictionary of 'stop words' to remove non-meaningful words such as articles 'a', 'an', and 'the'. We removed the 'space' character with an underscore in multi-word tokens for ease of procesing, and created a count table for the tokens in each paper. 

    Once the tokens in each paper were counted, we transformed the data into a sparse matrix format useable by the R package mikropml (@mikropml ref), using R packages caret and dplyr (@caret ref, @dplyr ref). Tokens were filtered to those which appear in greater than one paper. This allows comparison between papers by the model. We removed near zero variants (tokens with frequency very close to zero) as well as collapsing perfectly correlated tokens (tokens that always appear together) using R packages caret and mikropml to reduce model complexity. The data was then simplified to keep only the following variables; tokens, frequency, journal information, and hand identified NSD and DA variables. This simiplified sparse matrix data had the mean and standard deviation calculcated and saved for the frequency of each token to later apply a z-scoring method to future data to be predicted by the model. 

    Training of the DA and NSD Models
    We trained two random forest machine learning models using mikropml's "run_ml" function, one to determine if a paper contained new sequence data (NSD), and another to determine if the paper had data availabile (DA). The mikropml "run_ml" function uses methodology described by Topcuoglu et al (@topcuoglu2020) to split data for model training. Random forest models have one hyperparameter to tune, the mtry value. We began with mtry values of 100, 200, 300, 400, 500, and 600, to find peak hyperparameter performance given *N tokens*. We trained the models multiple times in accordance with existing methologies, first to find the optimal Area Under the Receiver-Operator Curve (AUROC) value for each model with N=100 seeds. Then to find the best mtry performance for each model, with N=1 seed. Finally, with N=1 seed to train each final model for use on experimental data.

    Preparation of the Experimental Dataset
    To fully answer our research questions, we created a larger database with N = 155779 papers curated from reference datasets Crossref, NCBI, Scopus, and the Web of Science (@crossref, @ncbi, @scopus, @wos). These papers span all twelve ASM journals of interest from the start of 2000 to the end of 2024. Once database was curated, we applied the same steps to ready papers for application of machine learning models as the model training datasets. See above for descriptions of webscraping html, cleaning html, removing unnecessary text, and creation of token count table for application in each of the machine learning models to determine the NSD and DA statuses for each paper. Once the frequency count tables were prepared for each paper, a z-score was applied using the saved data from each model appropriately, using the formula ((observed_token_frequency - model_token_frequency_mean)/(model_token_frequency_sd)). This z-scoring formula was applied to standardize the frequency of each token. Only tokens included in the machine learning models were retained in experimental datasets. Finally, each model was deployed on each paper to determine its NSD and DA status. 

    
    
    -   Creation of the training data
        -   Webscraping the HTML for the paper using wget
        -   Cleaning and of the HTML using r packages rvest and xml2 to get the desired portions of the paper from the HTML
            -   Want abstract, body of paper, tables with captions, and figure captions, as well as the side panels for all papers, but especially those containing the data availability statements in papers published after 2022
        -   Remove unnecessary text using r packages tm(text manipulation) and textstem
            -   Convert all text to lowercase
            -   Remove digits and non-alphabetic characters such as whitespace
            -   Lemmatize strings to trace them back to their root words and eliminate any possible issues with word tense, helps have the fewest number of unique words
        -   Creation of tokens and token count table for model using r package tokenizers and stopwords
            -   phrases of up to 1-3 consecutive words from the text of the paper
            -   removal of 'stopwords' from the Snowball dictionary, words that do not enhance the meaning of a sentence, a, an, the, then, etc
            -   replace space characters in multiple word tokens
            -   create a table showing which paper the word is from and how many times it occurs in the paper
        -   Unnest the data into the format required by mikropml (sparse matrix) using r packages caret, dplyr, and mikropml
            -   filter tokens for only those that appear in more than one paper
            -   remove near zero variants using caret and mikropml
            -   collapse perfectly correlated tokens into one token
            -   keep only tokens, number of occurences, hand identified variables (for training set), and journal information
            -   format into sparse matrix format, each paper is a row, each token frequency is a column 
            -   save mean and standarad deviation values for each token to apply z score normalization to experimental data
        -   Training model using r package mikropml's methodology using function run_ml
            -   80/20 split of data training/testing 100 times 
            -   second 80/20 training/testing within the first 80/20 split (from the 80) 100 times within each split
            -   reference the original paper (topcuoglu et al 2020)
            -   mtry values of 100, 200, 300, 400, 500, 600, to find peak hyperparameter performance given *N tokens*
            -   run_ml run multiple times
                -   first n=100 seeds per model to find average performance (see auroc graphs)
                -   second n=1 per model to find the best mtry performance
                -   third n=1 per model with best mtry to train final model for use on experimental data
    -   Use of the python tool snakemake to execute all code files (implemented in R)
    -   Preparation of the experimental dataset 
        -   Using multiple citation databases to ensure the most complete coverage of date range (2000-2024)
            -   N = **need to check ~ 149K**
            -   crossref
            -   web of science
            -   scopus
            -   ncbi
        -   See above for description of webscraping html, cleaning html, removing unnecessary text, and creation of token count table 
        -   Applying zscore using table of mean and standard deviation values from the training set
            -   formula (tokenFrequency - tokenMean/tokenSD)
            -   only tokens from the model were used in the testing set
        -   Using the each model to predict nsd and da status for new data
    - Statistical methodology 
        - negative binomial modeling using r package MASS
            - fixed modeling 
            - log transformation of age.in.months
            - 95% CI 
-   Supplemental Material file list (where applicable)
-   Acknowledgments
-   References
