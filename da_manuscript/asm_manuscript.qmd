---
title: "Data Accessibility Paper"

editor: visual

date: last-modified
date-format: medium
prefer-html: true

format:
  pdf:
    documentclass: article
    keep-tex: true
    lof: false
    toc: false
    number-sections: false
    output-file: asm_manuscript.pdf
    geometry: margin=1.0in
fontsize: 11pt
linestretch: 1.75
header-includes:
 - \usepackage[left]{lineno}
 - \linenumbers
 - \modulolinenumbers
 - \usepackage{helvet}
 - \renewcommand*\familydefault{\sfdefault}
 - \usepackage[T1]{fontenc}
execute:
  echo: FALSE
  tidy: TRUE
  eval: TRUE
  warning: FALSE
  message: FALSE
  cache: FALSE
runningtitle: 
runningauthor: Colovas

author:
  - name: Joanna Colovas
    orcid: 0009-0005-9180-442X
    affiliations:
      - ref: micro
  - name: Adena Collens
    affiliations:
      - ref: micro
  - name: Patrick D. Schloss
    orcid: 0000-0002-6935-4275
    email: pschloss@umich.edu
    corresponding: true
    affiliations:
      - ref: micro
      - ref: ccmb
affiliations:
  - id: micro
    name: Department of Microbiology & Immunology, University of Michigan
  - id: ccmb
    name: Center for Computational Medicine and Bioinformatics, University of Michigan
abstract: |
 
#importance: TODO
keywords: data accessibility, data reproducibility, supervised machine learning

bibliography: references.bib
csl: asm.csl
---

-   Abstract

-   Importance

    -   Incentivize authors to publish/make available their original data
    -   Publishing data helps get more use out of research
    -   Helps eliminate file drawer effect as it shows negative data

-   Keywords

    -   Data accessibility
    -   Data reproducibility

```{r}
#load data

#library statements
library(tidyverse)

training_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/new_groundtruth.csv")

all_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/predictions_with_metadata.csv.gz")

n_digits <- 2

# #nsd yes metadata needs to be updated at some point
# nsd_data <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/nsd_yes_metadata.csv.gz")


```

## Introduction

### Scientific Data as a Public Good

The United States Government spent over two hundred million dollars (USD) in 2024 on research expenditures (@congress2024). The result of all of these investments were data, paid for by taxpayers. Therefore, data are a public good. Public goods, for example public libraries, are able to be used by anyone without barrier to entry, and without diminishing the use of others. Data are best used for the benefit of those who provided the funds for it. Generated data are useful in many ways, not only by the original generators to analyze and answer study questions, but further used to answer additional questions on the same study system, to replicate the original analyses, and for meta-studies, by combining multiple similar datasets. A key tenet of the scientific method is this ability to replicate scientific findings to ensure that they are not due to error. Scientific findings can be replicated by re-completing the same analyses by another researcher, or by completing another type of analysis on the same data. This is only possible when the data used to complete the original analyses are available for use. Additionally, data are used to eliminate possible solutions to a problem by the publishing of negative or non-significant data. Thinking of negative data as a public good, their availability help researchers avoid sinking time and financial resources into investigation of non-viable hypotheses. Funding sources are not available to pursue the generation of negative data, and agencies look to support fruitful research. As a result, researchers have few incentives to publish negative or non-significant results. This lack of publication of non-fruitful investigation is more commonly known as the "file drawer effect" (@moniz_filedrawer_2025, @rosenthal1979file). In this current time period when government funding is uncertain, it is more important than ever to pursue fruitful research.

With the latest and greatest methodologies available across fields, increasing amounts of data are being generated each day, especially in the biological sciences where large and complex datasets are the new standard. ( @li_bigdata_2014, @pal_bigdata_2020). Availability of large quantities of study data and their associated metadata (data about data) are necessary resources for appropriate use and re-use of data, protocols, as well as recreation of analyses. Data availability are a deeply important component of the scientific process in the digital age, and the curation of digital records is a slowly emerging topic in data science( @howe_2008). Available data and analyses are the gold standard for recreation of studies and replication of their results. Not only is replication a worthy goal, but large datasets are often underutilized, and can continue to provide benefit and resources to researchers via their re-use towards investigating and answering further questions. As a result, the National Institutes of Health (NIH) has called for grant proposals for the creation, enhancement, and maintenance, of new and existing data repositories (@not_data_reuse, @par-23-2).

There are three major databases worldwide to support sequencing and sharing efforts. The National Library of Medicine's (NLM) National Center for Biotechnology (NCBI) in the United States, the Research Organization of Information Systems' (ROIS) National Institute of Genetics (NIG) in Japan, and the European Molecular Biology Lab's (EMBL) European Bioinformatics Institue (EBI) in Europe. These three databases are part of the International Nucleotide Sequence Database Collaboration (INSDC) (@indsc). Comparative genetics and genomics would not be possible without strong community commitment to data availability.

### American Society for Microbiology Journals (ASM)

The American Society for Microbiology(ASM) is the major professional body recognized by microbiologists. They have eighteen journals, thirteen primary research journals, three review journals, and two archive journals. In addition, several journals have been folded into others or renamed over time. In this study, we considered twelve of these journals, *Applied and Envrionmental Microbiology, Antimicrobial Agents and Chemotherapy, Infection and Immunity, Journal of Clinical Biology, Journal of Virology, Journal of Bacteriology, Journal of Microbiology and Biology Education, Microbiology Resource Announcments* (formerly known as *Genome Announcements*), *mSystems, mSphere, mBio, and Microbiology Spectrum.* Of note, several journals had changes to their publication goals during the 2000-2024 time period. In 2010, access to next generation sequencing (NGS) methods became commonplace, and there were a greater number of publications containing sequencing data, including full and partial genome announcments. The *Journal of Bacteriology* was the primary ASM journal to publish new genome announcements until 2013 when ASM announced journal *Genome Announcements* as a more permanent destination for this type of data. *Genome Announcements* was active from 2013 until 2018, when it was re-branded to *Microbiology Resource Announcements*, which has been active from 2018 until present. Another journal of note was *Microbiology Spectrum* and its re-brand. From 2013 until the fall of 2021, *Microbiology Spectrum* was a review journal. At this point and beyond, *Microbiology Spectrum* became a primary research journal (@asm_relaunch_2021). Several journals, *mBio* (b.2010), *Microbiology Spectrum* (b. 2013, re-brand 2021), *mSphere* (b. 2016), *mSystems* (b. 2016), and *Genome Announcements* (2013-2018) all did not span the entire study period of interest.

### Current Data Availability Policies

Current data availability guidelines have been informed by a number of policies created by funding agencies, peer-review journals, conference and special task groups, as well as community interest groups. In 2011, after the Future of Research Communication (FoRC) conference in Germany, scientists and others came together to establish FORCE11, a community interest group which sought to encourage and promote data availability standards (@force11). In 2014 the FORCE11 group published the Joint Declaration of Data Citation Principles (JDDCP), a document with continued work towards the standardization of data citation and future availability(@altman_2015). Some of the JDDCPs included crediting the authors of the data, providing data with unique identifiers, and the persistence of available data.

Also in 2011, the Genomic Standards Consortium (GSC) published a set of standards in *Nature Biotechnology* to promote the publication of the "minimum information about a marker gene sequence" (MIMARKS) or "minimum information about x sequence" (MIxS) (@yilmaz_minimum_2011). These standards are checklists usable by data generators and uploaders towards inclusion of relevant data with sequence uploads in the International Nucleotide Sequence Database Collaboration (INSDC). Some checklist items include if the data were published to an INSDC database and metadata about the study systems, data collected, and authors. An important factor was the ability to link the data to the results and to the data generators. The Findable, Accessible, Interoperable, and Reuseable (FAIR) data science guiding principles that were put forth in 2016 in *Nature Scientific Data* urges readers to "improve the infrastructure supporting the reuse of scholarly data" (@wilkinson_fair_2016). The FAIR principles were often cited by NIH in funding calls for strong data science practices(@not_data_reuse).

In 2021, a *Nature Medicine* publication put forth the "Strengthening the Organization and Reporting of Microbiome Studies" (STORMS) checklist to help authors self-identification of report-worthy elements of their data and metadata (@mirzayi_reporting_2021). Some items on the STORMS checklist included reporting the sequencing method used in the study, the study design, and physical location of the study. Unfortunately, none of these data availability principles or checklists were yet enforceable by any agency.

The National Institutes of Health (NIH) began enforcing the "Policy for Data Management and Sharing" (NOT-OD-21-013) in January of 2023, requiring all NIH funded studies to submit a data management and sharing plan (DMS) with their funding applications, and comply with their DMS plan after generation and publication of the funded work (@NIH_not-od-21-013_dms). A DMS plan includes detailed descriptions of data that will be generated in a study, related tools, standards, and data preservation plans. Non-compliance with NOT-OD-21-013 is identified by funding agencies during annual Research Performance Progress Reports (RPPRs), and may impact future funding decisions (@NIH_not-od-21-013_dms). The NIH policy for non-compliance with award terms and conditions varies due to the type of research misconduct, but is clear that the NIH will protect their own interests, including placing conditions on awards, preventing future awards, or closer monitoring of award activities. In this time of uncertain funding from the NIH and other funding agencies, it is more important that ever for investigators to maintain continued compliance. This compliance starts with readily available data and manuscripts.

At time of publication, the ASM journal program required that authors "make data fully available, without restriction, except in rare circumstances" (@Asm_opendata). They have adapted this policy from journals *Microbial Genomics* and *PLOS*. In the ASM open data policy they described the use of a "Data Availability Statement" which includes "data description, name(s) of the repositories, and digital object identifiers (DOIs) or accession numbers" and encouraged publishing data on relevant public repositories (@Asm_opendata). Consequences of non-compliance to the ASM open data policy included contacting research article authors to inform of non-compliance, publication of an "Expression of Concern" for the author and their compliance issues, future sanctions on publication in ASM journals, as well as contacting the affiliated research institution and/or funding agencies of the authors (@asm_ethics_2025). We endeavored to evaluate how well the microbiology community is using reproducible data practices as we believed that this group of researchers were early adopters of technologies available as a result of both the ASM and NIH policies towards data availability.

### Historical Nucleic Acid Sequencing Efforts and Examples

Beginning in 1996 with the International Strategy Meeting on Human Genome Sequencing in Bermuda, researchers have prioritized the release of all human genome sequencing information to "maximize its benefit to society" (@maxsonjones2018). The meeting participants agreed that "primary sequence data should be rapidly released", with "sequence assemblies \[to\] be released as soon as possible, in some centres\[sic\], assemblies of greater than 1 kb would be released automatically on a daily basis", and that "finished annotated sequence should be submitted immediately to public databases" (@maxsonjones2018). In 2003, another meeting, held in Ft. Lauderdale, FL, re-affirmed the 1996 Bermuda Principles, expanded upon them to apply more broadly towards sequencing data, and called for further support of these practices (@maxsonjones2018).

These foundational "Bermuda Principles" and "Ft. Launderdale Accords" agreements set the stage for both the Human Genome Project (HGP) and the Human Microbiome Project (HMP) to generate and share massive amounts of data over the course of their studies (@proctor_integrative_2019, @gevers_bioinformatics_2012, @group_evaluation_2012). The goal of the HMP was to sequence all body sites to determine the microbes found on and in the human body. Starting with major projects such as the HGP and HMP, nucleic acid sequencing efforts have been commonly uploaded and released using public databases. This allows for researchers to use and re-use the data from the HGP and HMP. Us of HMP sequencing data by researchers has resulted in over 650 scientific publications (@HMP_2025), and the completion of metadata studies, including those efforts participated in by these authors (@ding_dynamics_2014, @abubucker_metabolic_2012, @huttenhower_structure_2012).

An important tool for creating phylogenies is the NCBI Basic Local Alignment Search Tool (BLAST) (@altschul_blast), which is an essential tool for comparative research. The BLAST algorithm allows users to compare a nucleic acid or protein sequence to the NCBI database of over 1TB of data to find similar and related sequences. Without the upload of sequences to the NCBI database, the use and success of BLAST would not be possible, despite the effort required on part of the researcher to upload of sequences to one of the INSDC databases.

Availablity of data contributed to the rapid sequencing of the SARS-CoV-2 virus during the 2020 pandemic and subsequent expedition of vaccine development (@maxmen_2021).

## Results

```{r make_data_tables}

training_data_table <-
training_data %>% 
    count(container.title, new_seq_data, data_availability) %>%
    complete(., container.title, new_seq_data, data_availability, fill = list(`n` = 0L)) %>%
    mutate(.by = container.title, 
               n_total = sum(`n`), 
           n_fract = (n_total/sum(.$`n`)*100))  %>% 
    filter(new_seq_data == "Yes")  %>%
       mutate(.by = container.title,
           n_nsd = sum(`n`), 
           fract_nsd = n_nsd/n_total*100) %>%
    filter(data_availability == "Yes") %>% 
    mutate(.by = container.title, 
          n_da = `n`, 
          fract_da = n_da/n_nsd*100) %>%
    select(container.title, n_total:fract_da) # i think the NaN is fine for now

# kableExtra::kable(training_data_table, caption = "Summary Statistics of Model Training Dataset", 
#                 digits = n_digits)



 
#whole dataset yay!
whole_dataset_table <-
all_data %>% 
    count(container.title, nsd, da) %>%
    na.omit() %>%
    complete(., container.title, nsd, da, fill = list(`n` = 0L)) %>%
    mutate(.by = container.title, 
               n_total = sum(`n`), 
           n_fract = (n_total/sum(.$`n`)*100))  %>% 
    filter(nsd == "Yes")  %>%
       mutate(.by = container.title,
           n_nsd = sum(`n`), 
           fract_nsd = n_nsd/n_total*100) %>%
    filter(da == "Yes") %>% 
    mutate(.by = container.title, 
          n_da = `n`, 
          fract_da = n_da/n_nsd*100) %>%
    select(container.title, n_total:fract_da)

# kableExtra::kable(whole_dataset_table, caption = "Summary Statistics of All Data", 
#                 digits = n_digits)


t_total <- nrow(training_data)


```

```{r whole_dataset_stats}
#whole dataset



## whole dataset stats
n_total <- nrow(all_data)
all_per_nsd <- count(all_data, nsd) %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 3]]

all_per_da <- count(all_data, nsd, da) %>%
            filter(nsd == "Yes") %>%
            mutate(percent = `n`/sum(`n`)*100) %>%
            .[[2, 4]]


#year distribution - do we want a figure for this or the table? 
# do we want to combine this with the training set data? 
all_years <- 
   all_data %>% 
    count(., year.published) %>%
    rename(., n_whole_dataset = `n`)


#journal w/ highest and lowest rate of nsd
all_fract_high_nsd <- max(whole_dataset_table$fract_nsd)
all_j_high_nsd <- whole_dataset_table[[which.max(whole_dataset_table$fract_nsd), 1]]
all_fract_low_nsd <- min(whole_dataset_table$fract_nsd)
all_j_low_nsd <- whole_dataset_table[[which.min(whole_dataset_table$fract_nsd), 1]]

#journals w/ high/low rate da - two journals have 100% data availability
all_fract_high_da <- max(whole_dataset_table$fract_da, na.rm = TRUE)
all_j_high_da <- whole_dataset_table[whole_dataset_table$fract_da == all_fract_high_da, 1] %>%
               stringr::str_flatten(, collapse = " and ", na.rm = TRUE)

all_fract_low_da <- min(whole_dataset_table$fract_da, na.rm = TRUE)
all_j_low_da <- whole_dataset_table[[which.min(whole_dataset_table$fract_da), 1]]


## avg citations/paper
all_total_avg_cites <- all_data %>% 
    select(file, container.title, is.referenced.by.count) %>% 
    summarize(container.title = "Total",  
              mean_refs = mean(is.referenced.by.count, na.rm = TRUE), 
               median_refs = median(is.referenced.by.count, na.rm = TRUE))

all_avg_cites_j <- all_data %>% 
    select(file, container.title, is.referenced.by.count) %>% 
    summarize(.by = container.title, 
              mean_refs = mean(is.referenced.by.count, na.rm = TRUE), 
               median_refs = median(is.referenced.by.count, na.rm = TRUE))
all_avg_cites <- rbind(all_avg_cites_j, all_total_avg_cites)

all_max_median_cites <- max(all_avg_cites_j$median_refs, na.rm = TRUE)
all_j_max_median_cites <- all_avg_cites_j[[which.max(all_avg_cites_j$median_refs), 1]]

all_min_median_cites <- min(all_avg_cites_j$median_refs, na.rm = TRUE)
all_j_min_median_cites <- all_avg_cites_j[[which.min(all_avg_cites_j$median_refs), 1]]
```

```{r}
# get N for each journal 
n_journal <- 
    all_data %>% 
        count(container.title)

```

#### Descriptive Statistics

Using the Crossref database of DOIs, with validation from the Web of Science, NCBI, and Scopus DOI databases, we downloaded `{r} n_total` unique records of papers published in ASM journals. All papers were published between 2000 and 2024. These papers came from *Applied and Envrionmental Microbiology (N = `{r} n_journal[[1,2]]`), Antimicrobial Agents and Chemotherapy (N = `{r} n_journal[[2,2]]`), Infection and Immunity (N = `{r} n_journal[[4,2]]`), Journal of Bacteriology (N = `{r} n_journal[[5,2]]`), Journal of Clinical Microbiology (N = `{r} n_journal[[6,2]]`), Journal of Microbiology and Biology Education (N = `{r} n_journal[[7,2]]`), Journal of Virology (N = `{r} n_journal[[8,2]]`), Microbiology Resource Announcments* (formerly known as *Genome Announcements `{r} n_journal[[3,2]] + n_journal[[9,2]]`), *Microbiology Spectrum (N = `{r} n_journal[[10,2]]`), mBio (N = `{r} n_journal[[11,2]]`), mSphere(N = `{r} n_journal[[12,2]]`), and mSystems (N = `{r} n_journal[[13,2]]`).*

#### Training Dataset

As this was a large number of papers to investigate by hand, we created a subset of data to train two machine learning models. This dataset was representative of the larger dataset in age of paper and journal of origin and initially contained 500 papers. During iterative model training, a subset of papers were hand validated after each completed cycle. These subsets were created using dplyr's slice_n() to obtain papers from each journal. Additional hand selected papers were validated to ensure robust model training. Validated papers were added to the training set as we identified gaps in the dataset. This provided a total of `{r} t_total` papers.

```{r trained_model_stats}
best_models <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/final/best_model_stats.csv")

```

#### Random Forest Modeling

Two random forest models were trained to predict if published scientific papers "contained new sequence data", and if the paper "had data available", one model for each variable. Each model was trained using the same set of data, the normalized number of times a word or set of words appears in each paper in the set. Briefly, the HTML content of each paper was cleaned to removed non-meaningful words such as "a, an, the", and separated into tokens, meaningful units of 1 - 3 words. These tokens were also modified to eliminate issues with word tense. For example "interest_importance" was a two word token that appeared in the model. These tokens were summed and normalized to each other and were presented to the model as a table with each row as a paper, and each column as the frequency of a specific token. See methods for more information on this process.

Other models such as generalized linear regression (GLM) and boosted trees (XGBoost) were evaluated, but were ultimately rejected in favor of the random forest model due to fit (XXdata not shown- need to re-generateXX). Random forest models were chosen by their high Area Under the Receiver Operator Curve (AUROC) value to aid in this classification problem as the creation of many decision trees helps to improve accuracy and precision.

These model trainings resulted in two different models to answer two different questions. The new sequencing data model used an mtry value of `{r} round(best_models[[1, 3]], n_digits)` had an Area Under the Curve(AUC) of `{r} round(best_models[[3, 3]], n_digits)` and an accuracy of `{r} round(best_models[[5, 3]], n_digits)`. The sensitivity of the new sequencing data model was `{r} round(best_models[[8,3]], n_digits)`, and the specificity of the model was `{r} round(best_models[[9,3]], n_digits)`. The data availability model used an mtry value of `{r} round(best_models[1,2], n_digits)` had an AUC of `{r} round(best_models[[3,2]], n_digits)` and an accuracy of `{r} round(best_models[[5,2]], n_digits)`. The sensitivity of the data availability model was `{r} round(best_models[[8,2]], n_digits)`, and the specificity of the model was `{r} round(best_models[[9,2]], n_digits)` (See supplement for more information on trained machine learning models). This showed us that the models fit the data well, and could provide classifications on new data with an expected error rate of less than 10%. We deemed this as acceptable, accounting for variability in papers and data, as well as the large size of the dataset on which we deployed the models.

#### Deploying on 150K+ Papers

After downloading the HTML content of each paper, we cleaned the HTML content and readied it to apply our machine learning models to classify each paper, in brief, by removing non-meaningful words, followed by counting and filtering tokens, and finishing in model-compatible matrix format. Overall, `{r} round(all_per_nsd, n_digits)`% of papers had new sequencing data. The journal with the highest rate of new sequencing data was *`{r} all_j_high_nsd`* at `{r} round(all_fract_high_nsd, n_digits)`%, and the lowest was *`{r} all_j_low_nsd`* at `{r} round(all_fract_low_nsd, n_digits)`%. This was expected as *Genome Announcements* and *Microbiology Resource Announcements* were the primary places for the publication of new sequence data for ASM journals. In 2013, *Genome Announcements* was created to house papers describing new genomic sequencing efforts, and these publications were redirected from the *Journal of Bacteriology*. In 2018, *Genome Announcments* was re-branded to *Microbiology Resource Announcements*, the permanent home of new sequencing efforts. This change in journal scope for the *Journal of Bacteriology* explained the change in percentage of new sequencing data found in the journal over time. The *Journal of Microbiology and Biology Education*, as expected, contained the lowest percentage of new sequencing data. The scope of this journal is mainly resources for educators at the high school and college levels, and we did not expect papers to contain new sequencing data. In many of the ASM journals, the percentages of new sequencing data did not change significantly over time as their scope did not change. See Figure XX for percentages of new sequencing data for each journal, and figure XXX for the trends over time. (XX? Put figures in panel together for nsd overall and nsd over time? ) XX Other questions: XX Why did AEM go up so much in 2013ish? XX Should we discuss the IAI 2012ish blurb? XX

![Percentage of papers with new sequencing data](../Figures/summary_stats/fract_nsd.png)

![Percentage of papers with new sequencing data over time](../Figures/summary_stats/time_nsd.png)

Of all papers with new sequencing data, `{r} round(all_per_da, n_digits)`% of papers had data available. The journal with the highest rate of data availability was *`{r} all_j_high_da`* at `{r} round(all_fract_high_da, n_digits)`%, and the lowest was *`{r} all_j_low_da`* at `{r} round(all_fract_low_da, n_digits)`%. Similarly to the new sequencing data, this was expected as *`{r} all_j_high_da`* publishes new genomic sequence data for ASM and makes the data available per journal requirements. While overall data availability rates appeared low, these values took into account all papers published over the entire time period of interest. When plotted over time, data availability rates trended upwards over time, with a greater fraction of papers with new sequencing data having made their data available. As before, the *Journal of Bacteriology* had an upward trajectory until the redirection of papers to *Genome Announcements* and *Microbiology Resource Announcements* in 2013 and beyond. In 2023, the NIH policy requiring a data management and sharing plan (DMS) went into effect. All of the ASM journals show an upward trend in data availability in 2023 and 2024 to comply with funding agency policy. The *Journal of Microbiology and Biology Education* published so few papers during this period with new sequencing data, that they did not publish any papers with data available. See figures XXX and XXX for percentages of new sequencing data papers with data availability for each journal and over time. XX??Want to investigate some of the weirdly high years for major disease outbreaks/policy changes/editor changesXXX

![Percentage of new sequencing papers with data available](../Figures/summary_stats/fract_da.png)

![Percentage of new sequencing papers with data available over time](../Figures/summary_stats/time_da.png)

### Regression Model using Negative Binomial Models

```{r neg_binomial, eval = FALSE}

neg_bin <- read_csv("/Users/jocolova/Documents/Schloss/Colovas_Data_Accessibility/Data/negative_binomial/nsd_yes_glmnb_coeftable.csv")

neg_bin_table <- 
   kableExtra::kable(neg_bin, caption = "Coefficients for Negative Binomial Modeling", 
    digits = n_digits) 

```

In this study we sought to investigate the effect of new sequencing data and data availability on the number of citations received by a given paper. We focused on new sequencing data papers to determine the effect of having data available. On average, papers in the dataset had a median of `{r} round(all_total_avg_cites$median_refs, n_digits)` citations/article. The journal with the highest median rate of citations/article was *`{r} all_j_max_median_cites`* at `{r} round(all_max_median_cites, n_digits)`, and the lowest was *`{r} all_j_min_median_cites`* at `{r} round(all_min_median_cites, n_digits)`. To further investigate the effects of time (age in months), the journal that a given paper was published in, and the data availability status, a negative binomial regression model was used to best describe our data (see methods). We also limited our analysis to the last ten years of data (age in months \<=120) to exclude the change in scope for *Journal of Bacteriology* and removed papers from before the re-brand of *Microbiology Spectrum*. Our model has shown that over time, new sequencing papers with data available have a greater number of citations than those that do not have their data available(fig XXX). This relationship holds true in each journal publishing primary research articles. In journals *mSystems and Infection and Immunity*, the confidence intervals overlap, but that phenomenon was attributed to the low number of papers from these two journals with no data available.

In figure XX, we calculated the ratio of number of citations for papers of similar age containing data vs those that did not contain data by journal. Over time, papers with data available received more citations than those without up to well over 2 times more citations in the *Journal of Clinical Microbiology*. In all journals, papers with data availability had a greater number of citations at time point 60 months after publication, if not sooner. During the time interval this ratio grew, demonstrating increased citations for papers with data available available over time compared to those without, and the difference grew over the time interval.

![](../Figures/negative_binomial/model_predicted_plot.png)

![](../Figures/negative_binomial/emmeans_contrast_plot.png)

## Discussion

Our study sought to investigate the compliance and impact of data availability and associated policies on the ASM journal program.
After training machine learning models to classify papers to determine if they contained new sequencing data and if each new sequencing paper had data available, we found that on average, more than half of papers with new sequencing data had data availability (`{r} round(all_per_da, n_digits)`%). More often than not, authors of new sequencing data papers were making their data publicly available. This was in line with 2023 NIH policy requiring that data be made available using a Data Management and Sharing Plan (DMS plan) outlined in the NIH's NOT-OD-21-013 (@NIH_not-od-21-013_dms). XXDo we want percent for 2024?XX 

Expectledly, *Microbiology Resource Announcements* had the highest rates of data availability in new sequencing papers. This journal published new genomic sequences and was required by ASM to make data available. Journals such as the *Journal of Microbiology and Biology Education* and *Infection and Immunity* which published fewer new sequencing data papers due to their specific subject matters, had lower rates of new sequencing data and data availability in their journals.

Once these questions were answered and papers were classified, we moved to statistical analyses to answer the further question, "how does making data available impact citation metrics over time?" We were interested in citation metrics as a concrete metric to examine how making data availability benefits researchers and as a possible incentive towards making data availability. Our hypotheses were that microbiologists would have fairy high rates of data availability given the field's reliance on comparative research, and that other fields such as immunology would have lowered rates of data availability, as well as the hope that papers with data available would provide a greater number of citations over time compared to those without data available.

Using a negative binomial model we found that over time, papers with data available received more citations than those without. For each journal, this pattern held true. The difference in citations between those with and without data available climbed to well over 1.5x the amount of citations in some journals (Fig XXXX. *Journal of Clinical Microbiology*). These differences in ratio of papers by journal may have been due to the scope of papers found in each journal.  

We found that the effect of data availability providing greater citations intensifies over time, with the greatest differences in citations occurring at the 108 months since publication time point. This was great news to us and to other manuscript authors, that simply making their data available could provide as much as 50% increase in citations over time. We hope that these data will help to incentivize authors to make data available.

We acknowledge the limitations of our study data, that by focusing only on papers published in the ASM journal program, our results will not be as generalizable. We understand that this relationship between data availability and citation metrics may not be as strong in other journal programs, but we hope with the newest NIH funding policies, these trends will continue to improve data availability. Another limitation is the availability of paper metadata from various databases, with the Crossref database having provided the most complete metadata available for each paper. We were also not able to track citation metrics for individual papers over time. Our database sources only had available citation metrics at the time of dataset download (February 2025), with no intermediate time points for any given paper. This left us unable to understand trends over time due to popularity or obsolescence of technique or results.

While we found that making data available in publications gave a citation advantage, we hope that is not the only reason authors choose to make their data available. The need for reproducibility in science and demand for large datasets are additional reasons, as well as the NIH funding requirement. We hope that authors recognize the need to contribute to the data "commons", to further work done by others, and that even their negative results have value and power to stop others from continuing down dead ends.

## Materials and Methods

-   need to put this stuff somewhere

To avoid overfitting the models, we trained each model multiple times, performing validations on a subset of data after each iteration. This allowed us to have a greater number of papers in the training dataset by adding these iteratively validated papers, as well as to have great accuracy and precision within our models.

### Preparation of the Larger Experimental Dataset

To fully answer our research questions, we created a larger dataset with N = 155779 papers curated from reference databases Crossref, NCBI, Scopus, and the Web of Science (@rcrossref, @sayers_ncbi_2020 , @rscopus, @wos). These papers span all twelve ASM journals of interest from January 1st, 2000 to December 31st, 2024. The ASM Journals of interest were *Applied and Envrionmental Microbiology; Antimicrobial Agents and Chemotherapy; Infection and Immunity; Journal of Clinical Biology; Journal of Virology; Journal of Bacteriology; Journal of Microbiology and Biology Education; Microbiology Resource Announcments* (formerly known as *Genome Announcements*); *mSystems; mSphere; mBio; and Microbiology Spectrum.* The data was updated as of February 10th, 2025 with all citation counts frozen at that date.

### Creation of the Training set

To train our random forest machine learning model, we first created an appropriate training data set. For our initial training set, we chose an initial set of papers from across each journal and the time period of interest, adding special emphasis to include papers that were part of our desired set of interest (i.e. contained published data) to ensure that our two models could adequately characterize each paper as a new sequencing paper and if it published raw sequencing data or not. After creating our initial dataset, it was necessary to identify the status of both variables by hand and determine if each paper contained "new sequencing data", and if each one had "data available". This was completed by opening each paper in an internet browser window, and searching for a "data availability" or similar statement. See Table 1XXX for specific cases and how each of these cases were identified for the purpose of this study.

| Scenario | new sequencing data Status | data availability Status |
|----------------------------------|-------------------|-------------------|
| Paper is not about generating new sequencing data | No | No |
| Paper is about generating new sequencing data but has no data available | Yes | No |
| Paper is about generating new sequencing data and has data available | Yes | Yes |
| Paper uses sequencing as a confirmation of experimental technique (i.e. confirmation of plasmid insertion) | No | No |
| Paper discusses new computational or experimental tools | No | No |
| Paper has microarray data | No | No |
| Papers using MLST ONLY | No | No |
| Papers using qPCR ONLY | No | No |
| Papers about protein sequencing that have nucleotide sequencing | Yes | Yes/No depending on data availability |
| Papers using iRNA techniques | No | No |
| Papers using pyrosequencing/454 techniques | Yes | Yes/No depending on data availability |

: Possible Data Scenarios

### Adding Additional Training Set Papers

After initial training of our random forest models, a random sampling of papers was collected for each journal using dplyr's slice_n() to audit the efficacy of the models (@dplyr). To audit the efficacy of the models, we hand identified the status of both variables of interest, new sequencing data and data availability. We looked for weaknesses in the models, and updated methodology to reflect important areas of interest. For example, in 2023 the ASM journals changed their formatting to include the data availability statement of a paper in a sidebar of the webpage. We identified this by noticing that all papers from journal *Microbiology Resource Announcements* from 2023-2024 were incorrectly characterized by the model as data availability = No. The sidebar of the webpage was not included in the text the model was considering, and code had to be updated to include all sidebar data for all papers. These improvements to the model created a larger and more comprehensive training set of N = `r t_total`. These validations allowed us to create confusion matrices for each model. Confusion matrices for the final version of each trained model are available in XXX table(XXsupplement?).

### Creation of the Training Data from Training dataset

To perform the computational steps required for these experiments, we used the python tool Snakemake ( @mölder_snakemake ), and the University of Michigan's high performance computing cluster (see acknowledgements). Using our selected papers from the training dataset, we downloaded the entirety of each paper's source HTML using the command line tool wget. This allowed us to use the source HTML multiple times for updated analyses without the need to re-query the ASM webservers numerous times. Next, we performed cleaning of the HTML using R packages rvest (@rvest), textstem( @textstem), and xml2 (@xml2) to get the desired portions of the paper from the HTML including the abstract, the body of paper, all tables and figures with captions, as well as the side panels for all papers, but especially those containing the data availability statements in papers published after the 2023 change in webpage format (see above). Then we removed unnecessary text using R packages tm(text manipulation)(@tm_article, @tm_manual) and textstem (@textstem), as well as converting all text to lowercase, and the removal of digits and non-alphabetic characters such as whitespace. To have the fewest number of unique words, we lemmatized (sort words by grouping inflected or variant forms of the same word) words to trace them back to their root words and eliminate any possible issues with word tense. After this, we created and counted our 'tokens', phrases of up to 1-3 consecutive words from the text of the paper using R package tokeinziers (@tokenizers). Towards the goal of the fewest meaningful number of words, we used the 'Snowball' (@stopwords) dictionary of 'stop words' to remove non-meaningful words such as articles 'a', 'an', and 'the'. We removed the 'space' character with an underscore in multi-word tokens for ease of processing, and created a count table for the tokens in each paper.

Once the tokens in each paper were counted, we transformed the data into a sparse matrix format useable by the R package mikropml (@mikropml), using R packages caret and dplyr (@caret, @dplyr, @tidyverse). Tokens were filtered to those which appear in greater than one paper. This allows comparison between papers by the model. We removed near zero variants (tokens with frequency very close to zero) as well as collapsing perfectly correlated tokens (tokens that always appear together) using R packages caret and mikropml to reduce model complexity. The data was then simplified to keep only the following variables; tokens, frequency, journal information, and hand identified new sequencing data and data availability variables. This simiplified sparse matrix data had the mean and standard deviation calculcated and saved for the frequency of each token to later apply a z-scoring method to future data to be predicted by the model.

### Training of the data availability and new sequencing data Models

We trained two random forest machine learning models using mikropml's "run_ml" function, one to determine if a paper contained new sequence data, and another to determine if the paper had data available. The mikropml "run_ml" function uses methodology described by \*Topcuoglu et al.\*(@topcuoglu_mikropml_2021) to split data for model training. Random forest models have one hyperparameter to tune, the mtry value. We began with mtry values of 100, 200, 300, 400, 500, and 600, to find peak hyperparameter performance given *N tokens*. We trained the models multiple times in accordance with existing methodologies, first to find the optimal Area Under the Receiver-Operator Curve (AUROC) value for each model with N=100 seeds. Then to find the best mtry performance for each model, with N=1 seed. Finally, with N=1 seed to train each final model for use on experimental data.

### Deploying the RF Models

Once our RF models were ready we applied the same steps to ready papers for application of machine learning models as the model training dataset. See above for descriptions of web scraping HTML, cleaning HTML, removing unnecessary text, and creation of token count table for application in each of the machine learning models to determine the new sequencing data and data availability statuses for each paper. Once the frequency count tables were prepared for each paper, a z-score was applied using the saved data from each model appropriately, using the formula XX $((Observed token frequency - Model token frequency \mu)/(model token frequency sd))$. This z-scoring formula was applied to standardize the frequency of each token. Only tokens included in the machine learning models were retained in experimental datasets. Finally, each model was deployed on each paper to determine its new sequencing data and data availability status.

### Statistical Methodology

All papers included in the regression data had new sequencing data. We focused on the continuous outcome of "number of citations" with predictor variables journal (categorical), age in months (continuous), and data availability status (dichotomous). Due to the number of citations being bell shaped with a long right tail (very few papers at advanced age with many citations, producing a flat but non-zero line), the model that best described our data was a negative binomial regression model. A negative binomial model is appropriate for data that begins at zero and has a long 'tail' of data, as well as has differing means per group. This model also includes a dispersion parameter, $\theta$ to describe the spread of the data. We applied a log transformation to our age variable (age.in.months) to make linear the relationship between time and number of citations received to help better describe the model relationship.

$$
N_(refs) = data availability + log(age.in.months) + journal + (journal*data availability) + (log(age.in.months)*data availability) + (journal*log(age.in.months)) + (log(age.in.months)*data availability*journal)
$$

In general, we found that new sequencing data papers that made data availability received more citations over time than those that did not. See figure XXXX for trends in each major journal.

After each random forest model was deployed on our experimental dataset, we used a negative binomial regression model using the R package MASS (@MASS). A negative binomial regression model allows us to investigate data where group means are different than the overall dataset mean. A log transformation of the time variable(age.in.months) was applied prior to estimating the statistical regression model to correct for the nature of time as compared to other variables in the dataset. After applying the negative binomial model, we calculated ratios of the estimated number of citations per paper over time by data availability status using R package emmeans (@emmeans). We also used R package sjPlot to estimate the 95% CI for estimated citations over time for each journal by data availability status (@sjPlot).

-   Supplemental Material file list (where applicable)
-   Acknowledgments
    -   The authors acknowledge lab members C. Armour, A. Mason, M. Coden, S. Lucas, and K. Sovacool for help hand-classifying papers.
    -   ***“This research was supported in part through computational resources and services provided by Advanced Research Computing at the University of Michigan, Ann Arbor.”*** [ARC's RRID is: SCR_027337](https://rrid.site/data/record/nlx_144509-1/SCR_027337/resolver?q=SCR_027337%2A&l=SCR_027337%2A&i=rrid:scr_027337)
-   References
-   Figures/Tables/stats to make/get
    -   table of conditions to add to the methods of classification?