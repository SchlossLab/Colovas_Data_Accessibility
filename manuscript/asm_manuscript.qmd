---
title: "Data Accessibility Paper"
author: "Joanna Colovas"
format: pdf
editor: visual
---

-   Abstract
-   Importance
    -   Incentivize authors to publish/make available their original
        data
    -   Publishing data helps get more use out of research
    -   Helps eliminate file drawer effect as it shows negative data
-   Keywords
    -   Data accessibility
    -   Data reproducibility
-   Introduction
    -   Why make data available? 
        -   Replication of studies
        -   Data is generated by public funding
        -   Ability to compare results across conditions
        -   Negative results can help avoid research costs aka filedrawer effect
        -   Ability to perform metadata studies 
        -   Ability to have a wide variety of analyses
            -   See human microbiome project data and publications (HMP)
        -   Examples of data availability helping people out
            -   COVID sequencing and data availability was essential to vaccine
            development
            -   Comparison of sequencing across conditions for not so popular model organisms?
                -   BLAST - does BLAST use everything in NCBI? 
                -   Tragedy of the commons? if no one uploads to NCBI then there's nothing to search
                and the tools are basically useless?

    -   Current data availability and reproducibility guidelines
        -   NIH funded research must make data available as of January 2023
        (Policy for Data Management and Sharing (NOT-OD-21-013))
            -   How do they enforce this? 
            -   Do they even enforce this? 
        -   FORCE11 standards
        -   FAIR principles
        -   JDCCP standards
        -   All of these standards are at least 5 years old
        -   None of these are really all that enforceable by any agency 
    
    -   DNA sequencing efforts are commonly uploaded to databases
         -   Comparison is really essential in this field
                    -   Phylogenetic trees are made by comparing similarity of sequences
                    -   Especially trying to identify new sequences by fitting the into existing phylogeny
        -   International Nucleotide Sequence Database Collaboration
            (INDSC) databases
            -   ROIS - NGL (Japan)
            -   EMBL - EBI (Europe)
            -   NLM - NCBI (USA)
        -   This is actually more work for the data generator to make sure that their data
        is uploaded somewhere


    -   ASM journals
        -   ASM is the major professional body of microbiologists
        -   They have 12 journals (list of journals)
        -   Data availability policies of the journal
        -   Enforcement of da policies of the journal
        -   Why are we starting here with these journals? 
            -   Microbiology research generates large amounts of data
            -   It's common to upload them to a secondary database and not just
            include the data in a publication
             -   Want to evaluate how well this community is using
            reproducible data analysis as a metric because we think they will be 
            early adopters of the technology

    -   Microbiology data 
        -   Why do people upload sequences?
        -   Incentives? 

    -   Goals
        -   Investigate metrics of making data publicly available in 12 ASM
        journals
        -   Using machine learning models 
        -   To see what kind of data availability uptake we have in sequencing 
        fields where there's a known repository
        
-   Results
    -   Models
    -   Model prediction results
    -   Confusion Matrices for each model
        -   Spot checking and error methodology
    -   Regression modeling
    -   Regression modeling/confusion matrices for papers that contain
        new sequence data
    -   Citations corrected for time
-   Discussion
    -   Making data available provides more citations per paper than not
        doing so.
    -   Allows for replication of studies
    -   Why bother doing this?
        -   What advantage does it give you as a data generator 
        -   Is it worth the work? 
    -   
-   Materials and Methods
    -   Original data set from Adena (which I think is downloaded from
        crossref)
    -   Hand identifying 500 papers for da and nsd status
    -   Training model using mikropml methodology
    -   Training of the models
    -   Picking the best model (glmnet, rf, xgbtree, picked rf)
    -   Hypertuning parameters (rf = mtry)
    -   Snakemake/python
    -   Crossref gathering of DOIs (146K)
    -   Webscrape using httr2/rcrossref/wget
    -   Cleaning of html of each indiviudal file
    -   Tokenizing, stemming/lemmitization
    -   Formatting/applying zscore (and replicating that for the rest of
        the datasets)
    -   Using the model to predict da/nsd for new data
-   Supplemental Material file list (where applicable)
-   Acknowledgments
-   References
