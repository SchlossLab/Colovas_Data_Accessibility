---
title: "Data Accessibility Paper"
author: "Joanna Colovas"
format: pdf
editor: visual
---

-   Abstract
-   Importance
    -   Incentivize authors to publish/make available their original
        data
    -   Publishing data helps get more use out of research
    -   Helps eliminate file drawer effect as it shows negative data
-   Keywords
    -   Data accessibility
    -   Data reproducibility
-   Introduction
    -   Why make data available? 
        -   Replication of studies
        -   Data is generated by public funding
        -   Ability to compare results across conditions
        -   Negative results can help avoid research costs aka filedrawer effect
        -   Ability to perform metadata studies 
        -   Ability to have a wide variety of analyses
            -   See human microbiome project data and publications (HMP)
        -   Examples of data availability helping people out
            -   COVID sequencing and data availability was essential to vaccine
            development
            -   Comparison of sequencing across conditions for not so popular model organisms?
                -   BLAST - does BLAST use everything in NCBI? 
                -   Tragedy of the commons? if no one uploads to NCBI then there's nothing to search and the tools are basically useless?

    -   Current data availability and reproducibility guidelines
        -   NIH funded research must make data available as of January 2023
        (Policy for Data Management and Sharing (NOT-OD-21-013))
            -   How do they enforce this? 
            -   Do they even enforce this? 
        -   FORCE11 standards
        -   FAIR principles
        -   JDCCP standards
        -   All of these standards are at least 5 years old
        -   None of these are really all that enforceable by any agency 
    
    -   DNA sequencing efforts are commonly uploaded to databases
         -   Comparison is really essential in this field
                    -   Phylogenetic trees are made by comparing similarity of sequences
                    -   Especially trying to identify new sequences by fitting the into existing phylogeny
        -   International Nucleotide Sequence Database Collaboration
            (INDSC) databases
            -   ROIS - NGL (Japan)
            -   EMBL - EBI (Europe)
            -   NLM - NCBI (USA)
        -   This is actually more work for the data generator to make sure that their data is uploaded somewhere


    -   ASM journals
        -   ASM is the major professional body of microbiologists
        -   They have 12 journals (list of journals)
        -   Data availability policies of the journal
        -   Enforcement of da policies of the journal
        -   Why are we starting here with these journals? 
            -   Microbiology research generates large amounts of data
            -   It's common to upload them to a secondary database and not just include the data in a publication
             -   Want to evaluate how well this community is using reproducible data analysis as a metric because we think they will be early adopters of the technology

    -   Microbiology data 
        -   Why do people upload sequences?
        -   Incentives? 

    -   Goals
        -   Investigate metrics of making data publicly available in 12 ASM journals
        -   Using machine learning models 
        -   To see what kind of data availability uptake we have in sequencing fields where there's a known repository
        
-   Results
    -   Models
        -   random forest modeling gave the best results
    -   Model prediction results
        -   AUROC
        -   hyperparameter tuning 
    -   Confusion Matrices for each model
        -   Spot checking and error methodology
    -   Regression modeling
        -   regression equations
        -   number of citations corrected for the month published
        -   Regression modeling/confusion matrices for only papers that contain new sequence data
    
-   Discussion
    -   Percent of ASM papers that have new sequence data available (nsd Yes, da Yes)
        -   would it be more informative by journal? 
    -   Making data available 
        -   provides more citations per paper than not doing so by *XXX* number
        -   Allows for replication of studies
    -   Why bother doing this?
        -   What advantage does it give you as a data generator 
        -   Is it worth the work? 
    -   
-   Materials and Methods
    -   Creation of the training set
        -   Original data set from Adena,
            -   downloaded from crossref when? *need to find*
                -   did i update the metadata when i re-pulled from crossref
            -   how did she choose these? this is my guess
                -   variety of the 12 journals, years represented
                -   date published from 2000 to 2024
                -   adding of additional papers to cover gaps in representative-ness of datasets as well as the change of format in 2023
        -   Must have metadata from crossref
        -   Hand identifying 500 papers for da and nsd status
            -   new_seq_data(nsd) = "Is this paper about new sequencing data that has been generated?"
            -   data_availability(da) = "Did this paper make sequencing data available online such as in NCBI, SRA, EMBL, etc?" 
            -   A paper must be new_seq_data == "Yes" to have data_availability == "Yes".
            -   Papers that use sequencing as a confirmation of experimental technique are nsd = No, da = No
            -   Papers that are about new computational or experimental tools (i.e. mothur) are nsd = No, da = No
            -   Papers using microarray data are nsd = No, da = No
            -   Papers using MLST are not in and of themselves sequencing papers and are nsd = No, da = No
            -   Papers using qPCR only also are nsd = No, da = No 
        -   Training of model for initial testing 
        -   Spot checking initial modeling and realizing there were errors 
            -   do we need to include this?
            -   Adding +150 *(need to check number)* from spot checking exercises to get to N = ***
    -   Creation of the actual training data
        -   Webscraping the HTML for the paper using wget
        -   Cleaning and of the HTML using r packages rvest and xml2 to get the desired portions of the paper from the HTML
            -   Want abstract, body of paper, tables with captions, and figure captions, as well as the side panels for all papers, but especially those containing the data availability statements in papers published after 2022
        -   Remove unnecessary text using r packages tm(text manipulation) and textstem
            -   Convert all text to lowercase
            -   Remove digits and non-alphabetic characters such as whitespace
            -   Lemmatize strings to trace them back to their root words and eliminate any possible issues with word tense, helps have the fewest number of unique words
        -   Creation of tokens and token count table for model using r package tokenizers and stopwords
            -   phrases of up to 1-3 consecutive words from the text of the paper
            -   removal of 'stopwords' from the Snowball dictionary, words that do not enhance the meaning of a sentence, a, an, the, then, etc
            -   create a table showing which paper the word is from and how many times it occurs in the paper
        -   Unnest the data into the format required by mikropml (sparse matrix) using caret and dplyr
            -   filter tokens for only those that appear in more than one paper
            -   remove near zero variants using caret 
            

    -   Training model using mikropml methodology
        -   80/20
        -   second 80/20 
        -   reference the original paper
    -   Training of the models
    -   Picking the best model (glmnet, rf, xgbtree, picked rf)
    -   Hypertuning parameters (rf = mtry)
    -   Snakemake/python
    -   Crossref gathering of DOIs (146K)
    -   Webscrape using httr2/rcrossref/wget
    -   Cleaning of html of each indiviudal file
    -   Tokenizing, stemming/lemmitization
    -   Formatting/applying zscore (and replicating that for the rest of
        the datasets)
    -   Using the model to predict da/nsd for new data
-   Supplemental Material file list (where applicable)
-   Acknowledgments
-   References
